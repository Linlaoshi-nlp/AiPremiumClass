import os
import fasttext
import jieba
import numpy as np
import torch
from torch.utils.tensorboard import SummaryWriter


def local_save(filename="data/Fasttext/rich_corpus.txt"):
    rich_corpus = [
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦æ–¹å‘ï¼Œå®ƒç ”ç©¶äººä¸è®¡ç®—æœºä¹‹é—´å¦‚ä½•è¿›è¡Œè¯­è¨€äº¤æµã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§åˆ©ç”¨ç®—æ³•ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å¹¶è¿›è¡Œé¢„æµ‹çš„æŠ€æœ¯ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘å¤„ç†ä¿¡æ¯çš„æ–¹å¼ã€‚",
        "åœ¨æ–‡æœ¬æŒ–æ˜ä¸­ï¼Œè¯å‘é‡èƒ½å¤Ÿå°†è¯è¯­è½¬æ¢ä¸ºå…·æœ‰è¯­ä¹‰çš„ç¨ å¯†å‘é‡è¡¨ç¤ºã€‚",
        "FastText ç”± Facebook AI ç ”ç©¶é™¢å¼€å‘ï¼Œå®ƒèƒ½ç”Ÿæˆæ›´ç»†ç²’åº¦çš„è¯åµŒå…¥è¡¨ç¤ºã€‚",
        "ä½¿ç”¨è¯å‘é‡å¯ä»¥è¿›è¡Œæƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»å’Œé—®ç­”ç³»ç»Ÿç­‰ä»»åŠ¡ã€‚",
        "TF-IDF æ˜¯ä¸€ç§ç»å…¸çš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•ï¼Œä½†ä¸èƒ½æ•æ‰è¯è¯­ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚",
        "Word2Vec å’Œ FastText çš„ä¸»è¦åŒºåˆ«åœ¨äºæ˜¯å¦è€ƒè™‘å­è¯ç»“æ„ã€‚",
        "åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå¤„ç†åŒä¹‰è¯å’Œæ­§ä¹‰æ˜¯æå‡ç³»ç»Ÿå‡†ç¡®æ€§çš„å…³é”®ã€‚",
        "æ„å»ºä¸­æ–‡è¯­æ–™æ—¶éœ€è¦è€ƒè™‘åˆ†è¯ã€åœç”¨è¯è¿‡æ»¤å’Œç®€ç¹ä½“ç»Ÿä¸€ã€‚",
        "è¯­æ–™è´¨é‡ç›´æ¥å½±å“è¯å‘é‡æ¨¡å‹çš„è¡¨ç°ï¼Œé«˜è´¨é‡è¯­æ–™èƒ½æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚",
        "è¯å‘é‡çš„å¯è§†åŒ–å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¯æ±‡ä¹‹é—´çš„è¯­ä¹‰åˆ†å¸ƒã€‚",
    ]
    # ä¿å­˜ä¸ºè®­ç»ƒæ–‡ä»¶
    with open(filename, "w", encoding="utf-8") as f:
        for line in rich_corpus:
            f.write(line + "\n")


def splitTexts(input_file_path, output_file_path):
    with open(input_file_path, "r", encoding="utf-8") as file:
        lines = file.readlines()

    tokenized_lines = []
    for line in lines:
        # å»é™¤è¡Œå°¾çš„æ¢è¡Œç¬¦
        line = line.strip()
        # ä½¿ç”¨ jieba è¿›è¡Œåˆ†è¯
        tokenized_line = " ".join(jieba.lcut(line))
        tokenized_lines.append(tokenized_line)

    with open(output_file_path, "w", encoding="utf-8") as file:
        for line in tokenized_lines:
            file.write(line + "\n")


def train_unsupervised(filename):
    # è®­ç»ƒæ¨¡å‹ï¼ˆSkip-gram æ¨¡å¼ï¼‰
    model = fasttext.train_unsupervised(
        input=filename,
        dim=300,
        epoch=30,
        lr=0.1,
        ws=5,
        minCount=1,
        model="skipgram",  # æˆ– "cbow"
    )
    # æŸ¥è¯¢æŒ‡å®šè¯æ±‡çš„è¯å‘é‡ï¼ˆé•¿åº¦ä¸º dimï¼‰
    vec = model.get_word_vector("æ—…è¡Œ")
    print(f"æ—…è¡Œ çš„è¯å‘é‡å‰5ç»´ï¼š{vec[:5]}")

    # # ä¿å­˜æ¨¡å‹ï¼ˆä¿å­˜ä¸º .bin æ ¼å¼ï¼‰
    # model.save_model("dist/Fasttext/fasttext_model.bin")

    # # åŠ è½½æ¨¡å‹
    # model = fasttext.load_model("dist/Fasttext/fasttext_model.bin")

    # æŸ¥è¯¢æœ€ç›¸ä¼¼çš„è¯
    similar = model.get_nearest_neighbors("æ—…è¡Œ", k=5)
    print("\nä¸ 'æ—…è¡Œ' æœ€ç›¸è¿‘çš„è¯ï¼š")
    for sim, word in similar:
        print(f"{word}ï¼šç›¸ä¼¼åº¦ {sim:.4f}")

    print("\n model.words lengths", len(model.words))

    # è¯å‘é‡å¯è§†åŒ–
    writer = SummaryWriter(log_dir="runs/fasttext")
    words_data = model.words
    embeddings = []
    for word in words_data:
        embeddings.append(model.get_word_vector(word))

    writer.add_embedding(torch.tensor(np.array(embeddings)), metadata=words_data)
    writer.close()

    print("\nğŸ“Š å·²ä¿å­˜è‡³ TensorBoardã€‚è¿è¡Œï¼štensorboard --logdir=runs/fasttext")


# è®­ç»ƒè¯å‘é‡ - https://zhuanlan.zhihu.com/p/575814154
if __name__ == "__main__":
    # filename = "data/Fasttext/rich_corpus.txt"
    # local_save(filename)

    filename = "data/Fasttext/the_wandering_earth.txt"
    # è¦æ£€æŸ¥çš„æ–‡ä»¶è·¯å¾„
    dist_path = "dist/Fasttext/the_wandering_earth.txt"

    # åˆ¤æ–­æ–‡ä»¶æˆ–ç›®å½•æ˜¯å¦å­˜åœ¨
    if os.path.exists(dist_path):
        print(f"{dist_path} å­˜åœ¨ã€‚")
    else:
        print(f"{dist_path} ä¸å­˜åœ¨ã€‚")
        splitTexts(filename, dist_path)

    train_unsupervised(dist_path)

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 导入库\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# 3. 设置随机种子保证可重复性\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 4. 设备配置\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 5. 自定义分词器和数据集类\n",
        "def cooking_tokenizer(text):\n",
        "    \"\"\"针对烹饪数据集的专用分词器\"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())  # 移除非字母数字字符并转为小写\n",
        "    tokens = text.split()\n",
        "    # 合并常见烹饪术语\n",
        "    cooking_terms = ['stir fry', 'baking powder', 'olive oil']\n",
        "    for term in cooking_terms:\n",
        "        text = text.replace(term, term.replace(' ', '_'))\n",
        "    return text.split()\n",
        "\n",
        "class CookingDataset(Dataset):\n",
        "    def __init__(self, filepath, max_len=200, min_word_count=5):\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "        self.max_len = max_len\n",
        "        self.word2idx = {}\n",
        "        self.label2idx = {}\n",
        "\n",
        "        # 读取数据并统计词频\n",
        "        self._load_data(filepath)\n",
        "\n",
        "        # 构建词汇表（过滤低频词）\n",
        "        self._build_vocab(min_word_count)\n",
        "\n",
        "        # 构建标签映射\n",
        "        self._build_label_map()\n",
        "\n",
        "        # 打印数据集统计信息\n",
        "        print(f\"\\n数据集统计:\")\n",
        "        print(f\"- 总样本数: {len(self.texts)}\")\n",
        "        print(f\"- 词汇表大小: {len(self.word2idx)}\")\n",
        "        print(f\"- 类别数量: {len(self.label2idx)}\")\n",
        "        print(f\"- 最长文本长度: {max(len(t.split()) for t in self.texts)}\")\n",
        "        print(f\"- 示例标签分布: {Counter(self.labels).most_common(5)}\")\n",
        "\n",
        "    def _load_data(self, filepath):\n",
        "        \"\"\"加载数据并初步处理\"\"\"\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    parts = line.strip().split(' ', 1)\n",
        "                    if len(parts) == 2:\n",
        "                        label = parts[0].replace('__label__', '')\n",
        "                        text = parts[1]\n",
        "                        self.texts.append(text)\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def _build_vocab(self, min_count):\n",
        "        \"\"\"构建词汇表\"\"\"\n",
        "        word_counts = Counter()\n",
        "        for text in self.texts:\n",
        "            word_counts.update(cooking_tokenizer(text))\n",
        "\n",
        "        # 过滤低频词\n",
        "        vocab = [word for word, count in word_counts.items() if count >= min_count]\n",
        "\n",
        "        # 添加特殊token\n",
        "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
        "        for idx, word in enumerate(vocab, start=2):\n",
        "            self.word2idx[word] = idx\n",
        "\n",
        "    def _build_label_map(self):\n",
        "        \"\"\"构建标签映射\"\"\"\n",
        "        unique_labels = sorted(list(set(self.labels)))\n",
        "        self.label2idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        self.idx2label = {idx: label for label, idx in self.label2idx.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # 文本转token id\n",
        "        tokens = cooking_tokenizer(text)\n",
        "        token_ids = [self.word2idx.get(token, self.word2idx['<unk>']) for token in tokens][:self.max_len]\n",
        "\n",
        "        # 填充或截断\n",
        "        if len(token_ids) < self.max_len:\n",
        "            token_ids += [self.word2idx['<pad>']] * (self.max_len - len(token_ids))\n",
        "        else:\n",
        "            token_ids = token_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(token_ids), torch.tensor(self.label2idx[label])\n",
        "\n",
        "# 6. 创建数据集\n",
        "dataset = CookingDataset('/content/cooking.stackexchange.txt', max_len=150)\n",
        "\n",
        "# 7. 分割数据集\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# 8. 创建数据加载器\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# 9. 模型定义\n",
        "class CookingClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, hidden_dim=128, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # 双向GRU比LSTM在烹饪数据集上表现更好\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, n_layers,\n",
        "                         batch_first=True, bidirectional=True, dropout=0.3)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # 初始化权重\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"初始化权重\"\"\"\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                if 'embedding' in name:\n",
        "                    nn.init.uniform_(param, -0.1, 0.1)\n",
        "                elif 'gru' in name:\n",
        "                    if 'weight_ih' in name:\n",
        "                        nn.init.xavier_uniform_(param)\n",
        "                    elif 'weight_hh' in name:\n",
        "                        nn.init.orthogonal_(param)\n",
        "                elif 'fc' in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))  # [batch, seq, embed]\n",
        "\n",
        "        gru_out, _ = self.gru(embedded)  # [batch, seq, hidden*2]\n",
        "\n",
        "        # 使用注意力机制聚合序列信息\n",
        "        weights = torch.softmax(torch.mean(gru_out, dim=2), dim=1)\n",
        "        weighted = torch.bmm(gru_out.transpose(1, 2), weights.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        return self.fc(self.dropout(weighted))\n",
        "\n",
        "# 10. 初始化模型\n",
        "model = CookingClassifier(\n",
        "    vocab_size=len(dataset.word2idx),\n",
        "    embed_dim=128,\n",
        "    num_classes=len(dataset.label2idx),\n",
        "    hidden_dim=128,\n",
        "    n_layers=2\n",
        ").to(device)\n",
        "\n",
        "# 11. 训练函数\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # 梯度裁剪防止爆炸\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), total_correct / len(loader.dataset)\n",
        "\n",
        "# 12. 评估函数\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Evaluating\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), total_correct / len(loader.dataset)\n",
        "\n",
        "# 13. 训练配置\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5)\n",
        "\n",
        "# 14. 训练循环\n",
        "num_epochs = 15\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # 保存最佳模型\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f\"New best model saved with accuracy {best_acc:.4f}\")\n",
        "\n",
        "# 15. 测试函数\n",
        "def predict(text, model, dataset, device='cuda'):\n",
        "    model.eval()\n",
        "    tokens = cooking_tokenizer(text)\n",
        "    token_ids = [dataset.word2idx.get(token, dataset.word2idx['<unk>']) for token in tokens][:dataset.max_len]\n",
        "\n",
        "    if len(token_ids) < dataset.max_len:\n",
        "        token_ids += [dataset.word2idx['<pad>']] * (dataset.max_len - len(token_ids))\n",
        "    else:\n",
        "        token_ids = token_ids[:dataset.max_len]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = torch.tensor([token_ids]).to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        return dataset.idx2label[predicted.item()]\n",
        "\n",
        "# 16. 测试示例\n",
        "test_questions = [\n",
        "    \"How to stop Xanthan Gum from clumping?\",\n",
        "    \"Can you pure without a food processor?\",\n",
        "    \"Why did my soufflé collapse?\",\n",
        "    \"How to make authentic Italian pasta sauce?\",\n",
        "    \"What's the difference between baking powder and baking soda?\"\n",
        "]\n",
        "\n",
        "print(\"\\n测试预测结果:\")\n",
        "for question in test_questions:\n",
        "    pred = predict(question, model, dataset, device)\n",
        "    print(f\"Q: {question}\\nA: Predicted category -> {pred}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqHk-InLO_ZM",
        "outputId": "71ee1a5b-1bfd-44c0-c0ba-397c97be4976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-694697c8f644>\", line 17, in <cell line: 0>\n",
            "    torch.manual_seed(SEED)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/random.py\", line 46, in manual_seed\n",
            "    return default_generator.manual_seed(seed)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/random.py:46: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  return default_generator.manual_seed(seed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "数据集统计:\n",
            "- 总样本数: 15404\n",
            "- 词汇表大小: 3046\n",
            "- 类别数量: 533\n",
            "- 最长文本长度: 32\n",
            "- 示例标签分布: [('baking', 1423), ('food-safety', 1152), ('substitutions', 810), ('equipment', 735), ('bread', 403)]\n",
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:04<00:00, 42.95it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 135.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.8278 | Train Acc: 0.0807\n",
            "Val Loss: 4.6731 | Val Acc: 0.0893\n",
            "New best model saved with accuracy 0.0893\n",
            "\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 57.89it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 138.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.4716 | Train Acc: 0.1091\n",
            "Val Loss: 4.3374 | Val Acc: 0.1207\n",
            "New best model saved with accuracy 0.1207\n",
            "\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 57.82it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 140.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 4.2044 | Train Acc: 0.1567\n",
            "Val Loss: 4.1213 | Val Acc: 0.1970\n",
            "New best model saved with accuracy 0.1970\n",
            "\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 54.45it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 106.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.9460 | Train Acc: 0.2226\n",
            "Val Loss: 3.8726 | Val Acc: 0.2639\n",
            "New best model saved with accuracy 0.2639\n",
            "\n",
            "Epoch 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 57.29it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 137.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.6763 | Train Acc: 0.2822\n",
            "Val Loss: 3.6691 | Val Acc: 0.3132\n",
            "New best model saved with accuracy 0.3132\n",
            "\n",
            "Epoch 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 57.31it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 138.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.4331 | Train Acc: 0.3191\n",
            "Val Loss: 3.5354 | Val Acc: 0.3356\n",
            "New best model saved with accuracy 0.3356\n",
            "\n",
            "Epoch 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 51.64it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 68.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.2163 | Train Acc: 0.3479\n",
            "Val Loss: 3.4141 | Val Acc: 0.3557\n",
            "New best model saved with accuracy 0.3557\n",
            "\n",
            "Epoch 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 55.27it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 136.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.0149 | Train Acc: 0.3803\n",
            "Val Loss: 3.2931 | Val Acc: 0.3781\n",
            "New best model saved with accuracy 0.3781\n",
            "\n",
            "Epoch 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 57.06it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 137.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.8151 | Train Acc: 0.4130\n",
            "Val Loss: 3.2308 | Val Acc: 0.4002\n",
            "New best model saved with accuracy 0.4002\n",
            "\n",
            "Epoch 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 55.02it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 106.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.6530 | Train Acc: 0.4463\n",
            "Val Loss: 3.1670 | Val Acc: 0.4125\n",
            "New best model saved with accuracy 0.4125\n",
            "\n",
            "Epoch 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 54.86it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 138.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.4979 | Train Acc: 0.4695\n",
            "Val Loss: 3.1791 | Val Acc: 0.4158\n",
            "New best model saved with accuracy 0.4158\n",
            "\n",
            "Epoch 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 56.58it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 137.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.3589 | Train Acc: 0.4867\n",
            "Val Loss: 3.1223 | Val Acc: 0.4310\n",
            "New best model saved with accuracy 0.4310\n",
            "\n",
            "Epoch 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 54.54it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 87.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2301 | Train Acc: 0.5107\n",
            "Val Loss: 3.1356 | Val Acc: 0.4314\n",
            "New best model saved with accuracy 0.4314\n",
            "\n",
            "Epoch 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 48.88it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 137.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1358 | Train Acc: 0.5235\n",
            "Val Loss: 3.1432 | Val Acc: 0.4421\n",
            "New best model saved with accuracy 0.4421\n",
            "\n",
            "Epoch 15/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 193/193 [00:03<00:00, 55.83it/s]\n",
            "Evaluating: 100%|██████████| 49/49 [00:00<00:00, 136.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.0167 | Train Acc: 0.5419\n",
            "Val Loss: 3.1965 | Val Acc: 0.4333\n",
            "\n",
            "测试预测结果:\n",
            "Q: How long should I bake chicken at 350°F?\n",
            "A: Predicted category -> food-safety\n",
            "\n",
            "Q: What's the best substitute for eggs in vegan baking?\n",
            "A: Predicted category -> substitutions\n",
            "\n",
            "Q: Why did my soufflé collapse?\n",
            "A: Predicted category -> refrigerator\n",
            "\n",
            "Q: How to make authentic Italian pasta sauce?\n",
            "A: Predicted category -> pasta\n",
            "\n",
            "Q: What's the difference between baking powder and baking soda?\n",
            "A: Predicted category -> baking\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions = [\n",
        "    \"How to stop Xanthan Gum from clumping?\",\n",
        "    \"Can you pure without a food processor?\",\n",
        "    \"What's the difference between gazpacho and normal soups?\",\n",
        "    \"How to make authentic Italian pasta sauce?\",\n",
        "    \"Would ground popcorn meal differ from regular corn meal?\"\n",
        "]\n",
        "\n",
        "print(\"\\n测试预测结果:\")\n",
        "for question in test_questions:\n",
        "    pred = predict(question, model, dataset, device)\n",
        "    print(f\"Q: {question}\\nA: Predicted category -> {pred}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ4Fr2vGQOyu",
        "outputId": "348636fd-a5be-47d1-be9e-7bbfea0eaa31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "测试预测结果:\n",
            "Q: How to stop Xanthan Gum from clumping?\n",
            "A: Predicted category -> rice\n",
            "\n",
            "Q: Can you pure without a food processor?\n",
            "A: Predicted category -> equipment\n",
            "\n",
            "Q: What's the difference between gazpacho and normal soups?\n",
            "A: Predicted category -> soup\n",
            "\n",
            "Q: How to make authentic Italian pasta sauce?\n",
            "A: Predicted category -> pasta\n",
            "\n",
            "Q: Would ground popcorn meal differ from regular corn meal?\n",
            "A: Predicted category -> juice\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
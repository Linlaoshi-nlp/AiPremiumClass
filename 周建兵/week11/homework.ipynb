{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:07:18.966078Z",
     "iopub.status.busy": "2025-06-16T09:07:18.965520Z",
     "iopub.status.idle": "2025-06-16T09:08:03.066700Z",
     "shell.execute_reply": "2025-06-16T09:08:03.065897Z",
     "shell.execute_reply.started": "2025-06-16T09:07:18.966047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 09:07:41.578420: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750064862.056564      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750064862.179406      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate  \n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:03.068758Z",
     "iopub.status.busy": "2025-06-16T09:08:03.067951Z",
     "iopub.status.idle": "2025-06-16T09:08:05.182355Z",
     "shell.execute_reply": "2025-06-16T09:08:05.181759Z",
     "shell.execute_reply.started": "2025-06-16T09:08:03.068738Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da815c0df3a469cb9fe2d4b2aabb5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/767 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac098e30f3846b1941d80f65abe4a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/4.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776d6b97c00a47c9a82d3c000d766cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/333k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ed402893a8456099e3acfbf96e144d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/46364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c541058abb6342b699e02fa4326b64a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbf3aea3ad74c2c9d0c5b15af21ff68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 46364\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 4365\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 4365\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载HuggingFace中的dataset\n",
    "ds = load_dataset('PassbyGrocer/msra-ner')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:05.183245Z",
     "iopub.status.busy": "2025-06-16T09:08:05.182975Z",
     "iopub.status.idle": "2025-06-16T09:08:05.187921Z",
     "shell.execute_reply": "2025-06-16T09:08:05.187246Z",
     "shell.execute_reply.started": "2025-06-16T09:08:05.183206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for items in ds['train']:\n",
    "    print(items['tokens'])\n",
    "    print(items['ner_tags'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:05.190486Z",
     "iopub.status.busy": "2025-06-16T09:08:05.189775Z",
     "iopub.status.idle": "2025-06-16T09:08:05.867812Z",
     "shell.execute_reply": "2025-06-16T09:08:05.867329Z",
     "shell.execute_reply.started": "2025-06-16T09:08:05.190461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2026b1a42cc4a19b3e703243a81fb74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bad9de8f5b24ec4a29f85e9c1921aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea63d94e3a04e7e8e52951fe656538e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ac42e71c5b4449b8f0829ac1a18abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokennizer = AutoTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:05.868596Z",
     "iopub.status.busy": "2025-06-16T09:08:05.868421Z",
     "iopub.status.idle": "2025-06-16T09:08:10.062712Z",
     "shell.execute_reply": "2025-06-16T09:08:10.062100Z",
     "shell.execute_reply.started": "2025-06-16T09:08:05.868582Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证tag标签数量\n",
    "tags_id = set()\n",
    "for items in ds['train']:\n",
    "    tags_id.update(items['ner_tags'])\n",
    "tags_id    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:10.063526Z",
     "iopub.status.busy": "2025-06-16T09:08:10.063335Z",
     "iopub.status.idle": "2025-06-16T09:08:10.067800Z",
     "shell.execute_reply": "2025-06-16T09:08:10.067237Z",
     "shell.execute_reply.started": "2025-06-16T09:08:10.063512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'PER', 'LOC', 'ORG'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:10.068786Z",
     "iopub.status.busy": "2025-06-16T09:08:10.068530Z",
     "iopub.status.idle": "2025-06-16T09:08:10.085494Z",
     "shell.execute_reply": "2025-06-16T09:08:10.084908Z",
     "shell.execute_reply.started": "2025-06-16T09:08:10.068764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-PER', 'I-PER']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:10.086290Z",
     "iopub.status.busy": "2025-06-16T09:08:10.086077Z",
     "iopub.status.idle": "2025-06-16T09:08:10.102567Z",
     "shell.execute_reply": "2025-06-16T09:08:10.101986Z",
     "shell.execute_reply.started": "2025-06-16T09:08:10.086275Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'ORG': 1, 'LOC': 2, 'PER': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:10.103665Z",
     "iopub.status.busy": "2025-06-16T09:08:10.103446Z",
     "iopub.status.idle": "2025-06-16T09:08:10.122789Z",
     "shell.execute_reply": "2025-06-16T09:08:10.122173Z",
     "shell.execute_reply.started": "2025-06-16T09:08:10.103649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n",
    "a = tokennizer(test, \n",
    "                           truncation=True,\n",
    "                           add_special_tokens=False, \n",
    "                           max_length=512, \n",
    "                           is_split_into_words=True,)\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:10.125199Z",
     "iopub.status.busy": "2025-06-16T09:08:10.124899Z",
     "iopub.status.idle": "2025-06-16T09:08:25.001137Z",
     "shell.execute_reply": "2025-06-16T09:08:25.000260Z",
     "shell.execute_reply.started": "2025-06-16T09:08:10.125186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29237fc1014548fe8c23ec1c188fc0e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdb3daf8f994072b2be8dc91aa91362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408a567ce13c4702b73941766dfe6ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_input_proc(item):\n",
    "    # 文本已经分为字符，且tag索引也已经提供\n",
    "    # 所以数据预处理反而简单\n",
    "    # 导入已拆分为字符的列表，需要设置参数is_split_into_words=True\n",
    "    input_data = tokennizer(item['tokens'], \n",
    "                           truncation=True,\n",
    "                           add_special_tokens=False, \n",
    "                           max_length=512, \n",
    "                           is_split_into_words=True,\n",
    "                           return_offsets_mapping=True)\n",
    "    labels = [lbl[:512] for lbl in item['ner_tags']]\n",
    "    input_data['labels'] = labels\n",
    "    return input_data\n",
    "ds1 = ds.map(data_input_proc, batched=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:25.002309Z",
     "iopub.status.busy": "2025-06-16T09:08:25.002012Z",
     "iopub.status.idle": "2025-06-16T09:08:25.007066Z",
     "shell.execute_reply": "2025-06-16T09:08:25.006548Z",
     "shell.execute_reply.started": "2025-06-16T09:08:25.002283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds1.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:25.008180Z",
     "iopub.status.busy": "2025-06-16T09:08:25.007931Z",
     "iopub.status.idle": "2025-06-16T09:08:25.662151Z",
     "shell.execute_reply": "2025-06-16T09:08:25.661520Z",
     "shell.execute_reply.started": "2025-06-16T09:08:25.008154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n",
      "        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n",
      "        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n",
      "         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n",
      "        1159, 8013]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])}\n"
     ]
    }
   ],
   "source": [
    "for item in ds1['train']:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:25.663236Z",
     "iopub.status.busy": "2025-06-16T09:08:25.662953Z",
     "iopub.status.idle": "2025-06-16T09:08:28.247944Z",
     "shell.execute_reply": "2025-06-16T09:08:28.247239Z",
     "shell.execute_reply.started": "2025-06-16T09:08:25.663198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862fd66550144d18a9b3c087f2069088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-chinese', \n",
    "                                                        num_labels=len(tags),\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:28.248988Z",
     "iopub.status.busy": "2025-06-16T09:08:28.248707Z",
     "iopub.status.idle": "2025-06-16T09:08:28.284666Z",
     "shell.execute_reply": "2025-06-16T09:08:28.284099Z",
     "shell.execute_reply.started": "2025-06-16T09:08:28.248965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"msra_ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "    num_train_epochs = 3,    # 训练 epoch\n",
    "    # save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "    per_device_train_batch_size=32,  # 训练批次\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to='tensorboard',  # 训练输出记录\n",
    "    eval_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:28.285572Z",
     "iopub.status.busy": "2025-06-16T09:08:28.285357Z",
     "iopub.status.idle": "2025-06-16T09:08:28.291695Z",
     "shell.execute_reply": "2025-06-16T09:08:28.291158Z",
     "shell.execute_reply.started": "2025-06-16T09:08:28.285555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# metric 方法\n",
    "def compute_metric(result):\n",
    "    # result 是一个tuple (predicts, labels)\n",
    "    \n",
    "    # 获取评估对象\n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predicts,labels = result\n",
    "    predicts = np.argmax(predicts, axis=2)\n",
    "    \n",
    "    # 准备评估数据\n",
    "    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    results = seqeval.compute(predictions=predicts, references=labels)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:28.292726Z",
     "iopub.status.busy": "2025-06-16T09:08:28.292415Z",
     "iopub.status.idle": "2025-06-16T09:08:28.318385Z",
     "shell.execute_reply": "2025-06-16T09:08:28.317845Z",
     "shell.execute_reply.started": "2025-06-16T09:08:28.292704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokennizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:28.319037Z",
     "iopub.status.busy": "2025-06-16T09:08:28.318883Z",
     "iopub.status.idle": "2025-06-16T09:08:29.396800Z",
     "shell.execute_reply": "2025-06-16T09:08:29.396271Z",
     "shell.execute_reply.started": "2025-06-16T09:08:28.319025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds1['train'],\n",
    "    eval_dataset=ds1['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:08:29.397683Z",
     "iopub.status.busy": "2025-06-16T09:08:29.397446Z",
     "iopub.status.idle": "2025-06-16T09:44:27.738010Z",
     "shell.execute_reply": "2025-06-16T09:44:27.737328Z",
     "shell.execute_reply.started": "2025-06-16T09:08:29.397659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2175' max='2175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2175/2175 35:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Org</th>\n",
       "      <th>Per</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.030118</td>\n",
       "      <td>{'precision': 0.8578607322325915, 'recall': 0.8984962406015038, 'f1': 0.8777084098420859, 'number': 1330}</td>\n",
       "      <td>{'precision': 0.932937478169752, 'recall': 0.9290434782608695, 'f1': 0.9309864064133844, 'number': 2875}</td>\n",
       "      <td>{'precision': 0.8201928530913216, 'recall': 0.8854868340477648, 'f1': 0.8515901060070672, 'number': 1633}</td>\n",
       "      <td>0.882539</td>\n",
       "      <td>0.909901</td>\n",
       "      <td>0.896011</td>\n",
       "      <td>0.990805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.028727</td>\n",
       "      <td>{'precision': 0.8747300215982722, 'recall': 0.9135338345864662, 'f1': 0.8937109231335051, 'number': 1330}</td>\n",
       "      <td>{'precision': 0.9562764456981664, 'recall': 0.943304347826087, 'f1': 0.9497461040098056, 'number': 2875}</td>\n",
       "      <td>{'precision': 0.9253731343283582, 'recall': 0.9491733006736068, 'f1': 0.9371221281741233, 'number': 1633}</td>\n",
       "      <td>0.928305</td>\n",
       "      <td>0.938164</td>\n",
       "      <td>0.933208</td>\n",
       "      <td>0.992787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.031377</td>\n",
       "      <td>{'precision': 0.8692253020611229, 'recall': 0.9195488721804511, 'f1': 0.8936792108147605, 'number': 1330}</td>\n",
       "      <td>{'precision': 0.956811797752809, 'recall': 0.9478260869565217, 'f1': 0.9522977459374453, 'number': 2875}</td>\n",
       "      <td>{'precision': 0.9409975669099757, 'recall': 0.9473361910593999, 'f1': 0.944156240463839, 'number': 1633}</td>\n",
       "      <td>0.931514</td>\n",
       "      <td>0.941247</td>\n",
       "      <td>0.936355</td>\n",
       "      <td>0.992834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa347d959364d3d9ab34fe30913347e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.8578607322325915, 'recall': 0.8984962406015038, 'f1': 0.8777084098420859, 'number': 1330}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.932937478169752, 'recall': 0.9290434782608695, 'f1': 0.9309864064133844, 'number': 2875}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8201928530913216, 'recall': 0.8854868340477648, 'f1': 0.8515901060070672, 'number': 1633}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8747300215982722, 'recall': 0.9135338345864662, 'f1': 0.8937109231335051, 'number': 1330}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9562764456981664, 'recall': 0.943304347826087, 'f1': 0.9497461040098056, 'number': 2875}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9253731343283582, 'recall': 0.9491733006736068, 'f1': 0.9371221281741233, 'number': 1633}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8692253020611229, 'recall': 0.9195488721804511, 'f1': 0.8936792108147605, 'number': 1330}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.956811797752809, 'recall': 0.9478260869565217, 'f1': 0.9522977459374453, 'number': 2875}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9409975669099757, 'recall': 0.9473361910593999, 'f1': 0.944156240463839, 'number': 1633}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2175, training_loss=0.02357312038027007, metrics={'train_runtime': 2157.9292, 'train_samples_per_second': 64.456, 'train_steps_per_second': 1.008, 'total_flos': 1.1644638621676104e+16, 'train_loss': 0.02357312038027007, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T09:48:45.738099Z",
     "iopub.status.busy": "2025-06-16T09:48:45.737528Z",
     "iopub.status.idle": "2025-06-16T09:48:46.109433Z",
     "shell.execute_reply": "2025-06-16T09:48:46.108842Z",
     "shell.execute_reply.started": "2025-06-16T09:48:45.738077Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-ORG',\n",
       "  'score': 0.9889332,\n",
       "  'index': 3,\n",
       "  'word': '北',\n",
       "  'start': 2,\n",
       "  'end': 3},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.99495465,\n",
       "  'index': 4,\n",
       "  'word': '京',\n",
       "  'start': 3,\n",
       "  'end': 4},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': 0.99496955,\n",
       "  'index': 5,\n",
       "  'word': '天',\n",
       "  'start': 4,\n",
       "  'end': 5},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9896348,\n",
       "  'index': 6,\n",
       "  'word': '安',\n",
       "  'start': 5,\n",
       "  'end': 6},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9926347,\n",
       "  'index': 7,\n",
       "  'word': '门',\n",
       "  'start': 6,\n",
       "  'end': 7}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipeline = pipeline('token-classification', 'msra_ner_train/checkpoint-2175')\n",
    "pipeline('我爱北京天安门')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

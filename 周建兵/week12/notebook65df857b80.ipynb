{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:48:57.903249Z",
     "iopub.status.busy": "2025-06-16T10:48:57.902968Z",
     "iopub.status.idle": "2025-06-16T10:49:05.111301Z",
     "shell.execute_reply": "2025-06-16T10:49:05.110430Z",
     "shell.execute_reply.started": "2025-06-16T10:48:57.903229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:05.113021Z",
     "iopub.status.busy": "2025-06-16T10:49:05.112780Z",
     "iopub.status.idle": "2025-06-16T10:49:34.191119Z",
     "shell.execute_reply": "2025-06-16T10:49:34.190577Z",
     "shell.execute_reply.started": "2025-06-16T10:49:05.112998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 10:49:18.536847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750070958.743019      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750070958.806518      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:34.192449Z",
     "iopub.status.busy": "2025-06-16T10:49:34.191782Z",
     "iopub.status.idle": "2025-06-16T10:49:36.674716Z",
     "shell.execute_reply": "2025-06-16T10:49:36.674165Z",
     "shell.execute_reply.started": "2025-06-16T10:49:34.192427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7946733df7894ae9b9459fc234cbe3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecadebfc7d3c48069ce0097f1bd1387d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:36.676509Z",
     "iopub.status.busy": "2025-06-16T10:49:36.676221Z",
     "iopub.status.idle": "2025-06-16T10:49:37.380072Z",
     "shell.execute_reply": "2025-06-16T10:49:37.379487Z",
     "shell.execute_reply.started": "2025-06-16T10:49:36.676492Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19e342a812c4061a57659bc2c3f04ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5825edb649a3431f9fd42aabfb30c775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eec4297502e451c8ad0c55da99fea60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:37.380890Z",
     "iopub.status.busy": "2025-06-16T10:49:37.380697Z",
     "iopub.status.idle": "2025-06-16T10:49:39.219139Z",
     "shell.execute_reply": "2025-06-16T10:49:39.218610Z",
     "shell.execute_reply.started": "2025-06-16T10:49:37.380875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b5bff4bcbc4313a0d8fe1e31fd9d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368d415f380e42058c97d04c573f4de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e22333c73f42b2aae1e56511398b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-a33d0e4276aef9b4.parquet:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437c8980479d40c3a493797010b1b5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-07f476b71c5edde6.parquet:   0%|          | 0.00/178k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1cf090ba6b491486bb4f87f86daf76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a48d9eeae904db9a5e51dbf1163ac33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('nlhappy/CLUE-NER')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:39.219963Z",
     "iopub.status.busy": "2025-06-16T10:49:39.219761Z",
     "iopub.status.idle": "2025-06-16T10:49:39.225982Z",
     "shell.execute_reply": "2025-06-16T10:49:39.225182Z",
     "shell.execute_reply.started": "2025-06-16T10:49:39.219946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '浙商银行企业信贷部叶老桂博士则从另一个角度对五道门槛进行了解读。叶老桂认为，对目前国内商业银行而言，',\n",
       " 'ents': [{'indices': [9, 10, 11],\n",
       "   'is_continuous': True,\n",
       "   'label': 'name',\n",
       "   'text': '叶老桂'},\n",
       "  {'indices': [0, 1, 2, 3],\n",
       "   'is_continuous': True,\n",
       "   'label': 'company',\n",
       "   'text': '浙商银行'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:39.227204Z",
     "iopub.status.busy": "2025-06-16T10:49:39.226879Z",
     "iopub.status.idle": "2025-06-16T10:49:39.366649Z",
     "shell.execute_reply": "2025-06-16T10:49:39.365998Z",
     "shell.execute_reply.started": "2025-06-16T10:49:39.227179Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-MOVIE', 'I-MOVIE', 'B-GOVERNMENT', 'I-GOVERNMENT', 'B-ADDRESS', 'I-ADDRESS', 'B-NAME', 'I-NAME', 'B-SCENE', 'I-SCENE', 'B-POSITION', 'I-POSITION', 'B-COMPANY', 'I-COMPANY', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-BOOK', 'I-BOOK', 'B-GAME', 'I-GAME']\n",
      "\n",
      "{'O': 0, 'movie': 1, 'government': 2, 'address': 3, 'name': 4, 'scene': 5, 'position': 6, 'company': 7, 'organization': 8, 'book': 9, 'game': 10}\n"
     ]
    }
   ],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "           'company', 'scene', 'book', 'organization', 'government'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "print(tags)\n",
    "print()\n",
    "print(entity_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:39.367612Z",
     "iopub.status.busy": "2025-06-16T10:49:39.367393Z",
     "iopub.status.idle": "2025-06-16T10:49:40.681845Z",
     "shell.execute_reply": "2025-06-16T10:49:40.681241Z",
     "shell.execute_reply.started": "2025-06-16T10:49:39.367596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88ba54b2d7c4681a8b8e817b52e1113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae079ac1ecc4284a74ec2ac1b0e2851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entity_tags_proc(item):\n",
    "    # item即是dataset中记录\n",
    "    text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "    tags = [0] * text_len    # 初始值为‘O’\n",
    "    # 遍历实体列表，所有实体类别标记填入tags\n",
    "    entites = item['ents']\n",
    "    for ent in entites:\n",
    "        indices = ent['indices']  # 实体索引\n",
    "        label = ent['label']   # 实体名\n",
    "        tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "        for idx in indices[1:]:\n",
    "            tags[idx] = entity_index[label] * 2\n",
    "    return {'ent_tag': tags}\n",
    "\n",
    "# 使用自定义回调函数处理数据集记录\n",
    "ds1 = ds.map(entity_tags_proc)\n",
    "\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:40.682777Z",
     "iopub.status.busy": "2025-06-16T10:49:40.682531Z",
     "iopub.status.idle": "2025-06-16T10:49:47.005829Z",
     "shell.execute_reply": "2025-06-16T10:49:47.005249Z",
     "shell.execute_reply.started": "2025-06-16T10:49:40.682751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd63bcae30554c4b9dfb733558398f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732a55ce0841482a9c7aa7efef6a66ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1343 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10748\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'ents', 'ent_tag', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1343\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_input_proc(item):\n",
    "    # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "    batch_texts = [list(text) for text in item['text']]\n",
    "    # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "    input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n",
    "                           is_split_into_words=True, padding='max_length')\n",
    "    input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "    return input_data\n",
    "    \n",
    "\n",
    "ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:47.007910Z",
     "iopub.status.busy": "2025-06-16T10:49:47.007653Z",
     "iopub.status.idle": "2025-06-16T10:49:47.013314Z",
     "shell.execute_reply": "2025-06-16T10:49:47.012733Z",
     "shell.execute_reply.started": "2025-06-16T10:49:47.007896Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 512 512 512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in ds2['train']:\n",
    "    print(len(item['input_ids']), len(item['token_type_ids']), len(item['attention_mask']), len(item['labels']))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:47.014272Z",
     "iopub.status.busy": "2025-06-16T10:49:47.014016Z",
     "iopub.status.idle": "2025-06-16T10:49:49.735719Z",
     "shell.execute_reply": "2025-06-16T10:49:49.734985Z",
     "shell.execute_reply.started": "2025-06-16T10:49:47.014250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 记录转换为pytorch\n",
    "ds2.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:49.736714Z",
     "iopub.status.busy": "2025-06-16T10:49:49.736446Z",
     "iopub.status.idle": "2025-06-16T10:49:49.753437Z",
     "shell.execute_reply": "2025-06-16T10:49:49.752664Z",
     "shell.execute_reply.started": "2025-06-16T10:49:49.736684Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert.embeddings.word_embeddings.weight',\n",
       " 'bert.embeddings.position_embeddings.weight',\n",
       " 'bert.embeddings.token_type_embeddings.weight',\n",
       " 'bert.embeddings.LayerNorm.weight',\n",
       " 'bert.embeddings.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.attention.self.query.weight',\n",
       " 'bert.encoder.layer.0.attention.self.query.bias',\n",
       " 'bert.encoder.layer.0.attention.self.key.weight',\n",
       " 'bert.encoder.layer.0.attention.self.key.bias',\n",
       " 'bert.encoder.layer.0.attention.self.value.weight',\n",
       " 'bert.encoder.layer.0.attention.self.value.bias',\n",
       " 'bert.encoder.layer.0.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.0.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.0.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.0.output.dense.weight',\n",
       " 'bert.encoder.layer.0.output.dense.bias',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.attention.self.query.weight',\n",
       " 'bert.encoder.layer.1.attention.self.query.bias',\n",
       " 'bert.encoder.layer.1.attention.self.key.weight',\n",
       " 'bert.encoder.layer.1.attention.self.key.bias',\n",
       " 'bert.encoder.layer.1.attention.self.value.weight',\n",
       " 'bert.encoder.layer.1.attention.self.value.bias',\n",
       " 'bert.encoder.layer.1.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.1.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.1.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.1.output.dense.weight',\n",
       " 'bert.encoder.layer.1.output.dense.bias',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.attention.self.query.weight',\n",
       " 'bert.encoder.layer.2.attention.self.query.bias',\n",
       " 'bert.encoder.layer.2.attention.self.key.weight',\n",
       " 'bert.encoder.layer.2.attention.self.key.bias',\n",
       " 'bert.encoder.layer.2.attention.self.value.weight',\n",
       " 'bert.encoder.layer.2.attention.self.value.bias',\n",
       " 'bert.encoder.layer.2.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.2.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.2.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.2.output.dense.weight',\n",
       " 'bert.encoder.layer.2.output.dense.bias',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.attention.self.query.weight',\n",
       " 'bert.encoder.layer.3.attention.self.query.bias',\n",
       " 'bert.encoder.layer.3.attention.self.key.weight',\n",
       " 'bert.encoder.layer.3.attention.self.key.bias',\n",
       " 'bert.encoder.layer.3.attention.self.value.weight',\n",
       " 'bert.encoder.layer.3.attention.self.value.bias',\n",
       " 'bert.encoder.layer.3.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.3.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.3.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.3.output.dense.weight',\n",
       " 'bert.encoder.layer.3.output.dense.bias',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.attention.self.query.weight',\n",
       " 'bert.encoder.layer.4.attention.self.query.bias',\n",
       " 'bert.encoder.layer.4.attention.self.key.weight',\n",
       " 'bert.encoder.layer.4.attention.self.key.bias',\n",
       " 'bert.encoder.layer.4.attention.self.value.weight',\n",
       " 'bert.encoder.layer.4.attention.self.value.bias',\n",
       " 'bert.encoder.layer.4.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.4.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.4.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.4.output.dense.weight',\n",
       " 'bert.encoder.layer.4.output.dense.bias',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.attention.self.query.weight',\n",
       " 'bert.encoder.layer.5.attention.self.query.bias',\n",
       " 'bert.encoder.layer.5.attention.self.key.weight',\n",
       " 'bert.encoder.layer.5.attention.self.key.bias',\n",
       " 'bert.encoder.layer.5.attention.self.value.weight',\n",
       " 'bert.encoder.layer.5.attention.self.value.bias',\n",
       " 'bert.encoder.layer.5.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.5.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.5.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.5.output.dense.weight',\n",
       " 'bert.encoder.layer.5.output.dense.bias',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.attention.self.query.weight',\n",
       " 'bert.encoder.layer.6.attention.self.query.bias',\n",
       " 'bert.encoder.layer.6.attention.self.key.weight',\n",
       " 'bert.encoder.layer.6.attention.self.key.bias',\n",
       " 'bert.encoder.layer.6.attention.self.value.weight',\n",
       " 'bert.encoder.layer.6.attention.self.value.bias',\n",
       " 'bert.encoder.layer.6.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.6.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.6.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.6.output.dense.weight',\n",
       " 'bert.encoder.layer.6.output.dense.bias',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.attention.self.query.weight',\n",
       " 'bert.encoder.layer.7.attention.self.query.bias',\n",
       " 'bert.encoder.layer.7.attention.self.key.weight',\n",
       " 'bert.encoder.layer.7.attention.self.key.bias',\n",
       " 'bert.encoder.layer.7.attention.self.value.weight',\n",
       " 'bert.encoder.layer.7.attention.self.value.bias',\n",
       " 'bert.encoder.layer.7.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.7.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.7.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.7.output.dense.weight',\n",
       " 'bert.encoder.layer.7.output.dense.bias',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.attention.self.query.weight',\n",
       " 'bert.encoder.layer.8.attention.self.query.bias',\n",
       " 'bert.encoder.layer.8.attention.self.key.weight',\n",
       " 'bert.encoder.layer.8.attention.self.key.bias',\n",
       " 'bert.encoder.layer.8.attention.self.value.weight',\n",
       " 'bert.encoder.layer.8.attention.self.value.bias',\n",
       " 'bert.encoder.layer.8.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.8.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.8.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.8.output.dense.weight',\n",
       " 'bert.encoder.layer.8.output.dense.bias',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.attention.self.query.weight',\n",
       " 'bert.encoder.layer.9.attention.self.query.bias',\n",
       " 'bert.encoder.layer.9.attention.self.key.weight',\n",
       " 'bert.encoder.layer.9.attention.self.key.bias',\n",
       " 'bert.encoder.layer.9.attention.self.value.weight',\n",
       " 'bert.encoder.layer.9.attention.self.value.bias',\n",
       " 'bert.encoder.layer.9.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.9.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.9.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.9.output.dense.weight',\n",
       " 'bert.encoder.layer.9.output.dense.bias',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.attention.self.query.weight',\n",
       " 'bert.encoder.layer.10.attention.self.query.bias',\n",
       " 'bert.encoder.layer.10.attention.self.key.weight',\n",
       " 'bert.encoder.layer.10.attention.self.key.bias',\n",
       " 'bert.encoder.layer.10.attention.self.value.weight',\n",
       " 'bert.encoder.layer.10.attention.self.value.bias',\n",
       " 'bert.encoder.layer.10.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.10.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.10.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.10.output.dense.weight',\n",
       " 'bert.encoder.layer.10.output.dense.bias',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.attention.self.query.weight',\n",
       " 'bert.encoder.layer.11.attention.self.query.bias',\n",
       " 'bert.encoder.layer.11.attention.self.key.weight',\n",
       " 'bert.encoder.layer.11.attention.self.key.bias',\n",
       " 'bert.encoder.layer.11.attention.self.value.weight',\n",
       " 'bert.encoder.layer.11.attention.self.value.bias',\n",
       " 'bert.encoder.layer.11.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.11.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.11.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.11.output.dense.weight',\n",
       " 'bert.encoder.layer.11.output.dense.bias',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.bias',\n",
       " 'classifier.weight',\n",
       " 'classifier.bias']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, params in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:50:10.642593Z",
     "iopub.status.busy": "2025-06-16T10:50:10.642316Z",
     "iopub.status.idle": "2025-06-16T10:50:10.647524Z",
     "shell.execute_reply": "2025-06-16T10:50:10.646837Z",
     "shell.execute_reply.started": "2025-06-16T10:50:10.642573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%writefile ner_ddp.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 数据集\n",
    "    ds = load_dataset('nlhappy/CLUE-NER')\n",
    "    # entity_index\n",
    "    entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "               'company', 'scene', 'book', 'organization', 'government'})\n",
    "    tags = ['O']\n",
    "    for entity in entites[1:]:\n",
    "        tags.append('B-' + entity.upper())\n",
    "        tags.append('I-' + entity.upper())\n",
    "    \n",
    "    entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "    \n",
    "    def entity_tags_proc(item):\n",
    "        # item即是dataset中记录\n",
    "        text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "        tags = [0] * text_len    # 初始值为‘O’\n",
    "        # 遍历实体列表，所有实体类别标记填入tags\n",
    "        entites = item['ents']\n",
    "        for ent in entites:\n",
    "            indices = ent['indices']  # 实体索引\n",
    "            label = ent['label']   # 实体名\n",
    "            tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "            for idx in indices[1:]:\n",
    "                tags[idx] = entity_index[label] * 2\n",
    "        return {'ent_tag': tags}\n",
    "    \n",
    "    # 使用自定义回调函数处理数据集记录\n",
    "    ds1 = ds.map(entity_tags_proc)\n",
    "    \n",
    "    def data_input_proc(item):\n",
    "        # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "        batch_texts = [list(text) for text in item['text']]\n",
    "        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "        input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n",
    "                               is_split_into_words=True, padding='max_length')\n",
    "        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "        return input_data\n",
    "        \n",
    "    \n",
    "    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n",
    "    \n",
    "    \n",
    "    local_rank = rank\n",
    "    \n",
    "    id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "    lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                            num_labels=21,\n",
    "                                                            id2label=id2lbl,\n",
    "                                                            label2id=lbl2id)\n",
    "    model.to(local_rank)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "        num_train_epochs = 3,    # 训练 epoch\n",
    "        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "        per_device_train_batch_size=8,  # 训练批次\n",
    "        per_device_eval_batch_size=8,\n",
    "        report_to='tensorboard',  # 训练输出记录\n",
    "        eval_strategy=\"epoch\",\n",
    "        local_rank=local_rank,   # 当前进程 RANK\n",
    "        fp16=True,               # 使用混合精度\n",
    "        lr_scheduler_type='linear',  # 动态学习率\n",
    "        warmup_steps=100,        # 预热步数\n",
    "        ddp_find_unused_parameters=False  # 优化DDP性能\n",
    "    )\n",
    "    \n",
    "    def compute_metric(result):\n",
    "        # result 是一个tuple (predicts, labels)\n",
    "        \n",
    "        # 获取评估对象\n",
    "        seqeval = evaluate.load('seqeval')\n",
    "        predicts,labels = result\n",
    "        predicts = np.argmax(predicts, axis=2)\n",
    "        \n",
    "        # 准备评估数据\n",
    "        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                     for ps,ls in zip(predicts,labels)]\n",
    "        labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                     for ps,ls in zip(predicts,labels)]\n",
    "        results = seqeval.compute(predictions=predicts, references=labels)\n",
    "    \n",
    "        return results\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=ds2['train'],\n",
    "        eval_dataset=ds2['validation'],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metric\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:49:50.242168Z",
     "iopub.status.busy": "2025-06-16T10:49:50.241909Z",
     "iopub.status.idle": "2025-06-16T10:49:50.245981Z",
     "shell.execute_reply": "2025-06-16T10:49:50.245255Z",
     "shell.execute_reply.started": "2025-06-16T10:49:50.242147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T10:52:56.151204Z",
     "iopub.status.busy": "2025-06-16T10:52:56.150516Z",
     "iopub.status.idle": "2025-06-16T10:52:56.157859Z",
     "shell.execute_reply": "2025-06-16T10:52:56.157097Z",
     "shell.execute_reply.started": "2025-06-16T10:52:56.151175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ner_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ner_ddp.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "# 设置分布式环境\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# 清理分布式环境\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    # 数据集\n",
    "    ds = load_dataset('nlhappy/CLUE-NER')\n",
    "    # entity_index\n",
    "    entites = ['O'] + list({'movie', 'name', 'game', 'address', 'position', \\\n",
    "               'company', 'scene', 'book', 'organization', 'government'})\n",
    "    tags = ['O']\n",
    "    for entity in entites[1:]:\n",
    "        tags.append('B-' + entity.upper())\n",
    "        tags.append('I-' + entity.upper())\n",
    "    \n",
    "    entity_index = {entity:i for i, entity in enumerate(entites)}\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')\n",
    "    \n",
    "    def entity_tags_proc(item):\n",
    "        # item即是dataset中记录\n",
    "        text_len = len(item['text'])  # 根据文本长度生成tags列表\n",
    "        tags = [0] * text_len    # 初始值为‘O’\n",
    "        # 遍历实体列表，所有实体类别标记填入tags\n",
    "        entites = item['ents']\n",
    "        for ent in entites:\n",
    "            indices = ent['indices']  # 实体索引\n",
    "            label = ent['label']   # 实体名\n",
    "            tags[indices[0]] = entity_index[label] * 2 - 1\n",
    "            for idx in indices[1:]:\n",
    "                tags[idx] = entity_index[label] * 2\n",
    "        return {'ent_tag': tags}\n",
    "    \n",
    "    # 使用自定义回调函数处理数据集记录\n",
    "    ds1 = ds.map(entity_tags_proc)\n",
    "    \n",
    "    def data_input_proc(item):\n",
    "        # 输入文本先拆分为字符，再转换为模型输入的token索引\n",
    "        batch_texts = [list(text) for text in item['text']]\n",
    "        # 导入拆分为字符的文本列表时，需要设置参数is_split_into_words=True\n",
    "        input_data = tokenizer(batch_texts, truncation=True, add_special_tokens=False, max_length=512, \n",
    "                               is_split_into_words=True, padding='max_length')\n",
    "        input_data['labels'] = [tag + [0] * (512 - len(tag)) for tag in item['ent_tag']]\n",
    "        return input_data\n",
    "        \n",
    "    \n",
    "    ds2 = ds1.map(data_input_proc, batched=True)  # batch_size 1000\n",
    "    \n",
    "    \n",
    "    local_rank = rank\n",
    "    \n",
    "    id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "    lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', \n",
    "                                                            num_labels=21,\n",
    "                                                            id2label=id2lbl,\n",
    "                                                            label2id=lbl2id)\n",
    "    model.to(local_rank)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "        num_train_epochs = 3,    # 训练 epoch\n",
    "        save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "        per_device_train_batch_size=8,  # 训练批次\n",
    "        per_device_eval_batch_size=8,\n",
    "        report_to='tensorboard',  # 训练输出记录\n",
    "        eval_strategy=\"epoch\",\n",
    "        local_rank=local_rank,   # 当前进程 RANK\n",
    "        fp16=True,               # 使用混合精度\n",
    "        lr_scheduler_type='linear',  # 动态学习率\n",
    "        warmup_steps=100,        # 预热步数\n",
    "        ddp_find_unused_parameters=False  # 优化DDP性能\n",
    "    )\n",
    "    \n",
    "    def compute_metric(result):\n",
    "        # result 是一个tuple (predicts, labels)\n",
    "        \n",
    "        # 获取评估对象\n",
    "        seqeval = evaluate.load('seqeval')\n",
    "        predicts,labels = result\n",
    "        predicts = np.argmax(predicts, axis=2)\n",
    "        \n",
    "        # 准备评估数据\n",
    "        predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                     for ps,ls in zip(predicts,labels)]\n",
    "        labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                     for ps,ls in zip(predicts,labels)]\n",
    "        results = seqeval.compute(predictions=predicts, references=labels)\n",
    "    \n",
    "        return results\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=ds2['train'],\n",
    "        eval_dataset=ds2['validation'],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metric\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch-KMNIST训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导包及超参数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import KMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 超参数\n",
    "LR = 1e-3\n",
    "epochs = 100\n",
    "NERVES_NUM = 128\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "ITEM_SIZE = 28 * 28 \n",
    "LABEL_NUM = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_data = KMNIST(root='./k_data', train=True, download=True, transform=ToTensor())\n",
    "test_data = KMNIST(root='./k_data', train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集加载\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset KMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./k_data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set([clz for _, clz in train_data])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(ITEM_SIZE, NERVES_NUM),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(NERVES_NUM, LABEL_NUM)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 损失函数&优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "loss_fn = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "# 优化器(模型参数更新)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 2.306649923324585\n",
      "epoch: 1 loss: 2.277599573135376\n",
      "epoch: 2 loss: 2.2643184661865234\n",
      "epoch: 3 loss: 2.2619097232818604\n",
      "epoch: 4 loss: 2.249053478240967\n",
      "epoch: 5 loss: 2.2348663806915283\n",
      "epoch: 6 loss: 2.217611312866211\n",
      "epoch: 7 loss: 2.2055470943450928\n",
      "epoch: 8 loss: 2.1822102069854736\n",
      "epoch: 9 loss: 2.146639823913574\n",
      "epoch: 10 loss: 2.1657910346984863\n",
      "epoch: 11 loss: 2.1404030323028564\n",
      "epoch: 12 loss: 2.1123669147491455\n",
      "epoch: 13 loss: 2.104515790939331\n",
      "epoch: 14 loss: 2.094886064529419\n",
      "epoch: 15 loss: 2.0943572521209717\n",
      "epoch: 16 loss: 2.045344114303589\n",
      "epoch: 17 loss: 2.000697374343872\n",
      "epoch: 18 loss: 1.9773238897323608\n",
      "epoch: 19 loss: 1.9727787971496582\n",
      "epoch: 20 loss: 1.9098032712936401\n",
      "epoch: 21 loss: 1.9543732404708862\n",
      "epoch: 22 loss: 1.8721956014633179\n",
      "epoch: 23 loss: 1.8456753492355347\n",
      "epoch: 24 loss: 1.8198424577713013\n",
      "epoch: 25 loss: 1.8221564292907715\n",
      "epoch: 26 loss: 1.7671207189559937\n",
      "epoch: 27 loss: 1.772053837776184\n",
      "epoch: 28 loss: 1.7589988708496094\n",
      "epoch: 29 loss: 1.7031699419021606\n",
      "epoch: 30 loss: 1.7236579656600952\n",
      "epoch: 31 loss: 1.729435920715332\n",
      "epoch: 32 loss: 1.6707029342651367\n",
      "epoch: 33 loss: 1.6570520401000977\n",
      "epoch: 34 loss: 1.6840909719467163\n",
      "epoch: 35 loss: 1.6595630645751953\n",
      "epoch: 36 loss: 1.6018232107162476\n",
      "epoch: 37 loss: 1.6211391687393188\n",
      "epoch: 38 loss: 1.527833104133606\n",
      "epoch: 39 loss: 1.5948052406311035\n",
      "epoch: 40 loss: 1.4599789381027222\n",
      "epoch: 41 loss: 1.3751119375228882\n",
      "epoch: 42 loss: 1.4666870832443237\n",
      "epoch: 43 loss: 1.5035057067871094\n",
      "epoch: 44 loss: 1.3581466674804688\n",
      "epoch: 45 loss: 1.4166089296340942\n",
      "epoch: 46 loss: 1.4364231824874878\n",
      "epoch: 47 loss: 1.4655884504318237\n",
      "epoch: 48 loss: 1.4847393035888672\n",
      "epoch: 49 loss: 1.3833274841308594\n",
      "epoch: 50 loss: 1.439087986946106\n",
      "epoch: 51 loss: 1.2430243492126465\n",
      "epoch: 52 loss: 1.3067506551742554\n",
      "epoch: 53 loss: 1.245717167854309\n",
      "epoch: 54 loss: 1.1541335582733154\n",
      "epoch: 55 loss: 1.3010849952697754\n",
      "epoch: 56 loss: 1.405895709991455\n",
      "epoch: 57 loss: 1.2330721616744995\n",
      "epoch: 58 loss: 1.2490482330322266\n",
      "epoch: 59 loss: 1.3365998268127441\n",
      "epoch: 60 loss: 1.0598562955856323\n",
      "epoch: 61 loss: 1.265881896018982\n",
      "epoch: 62 loss: 1.0749844312667847\n",
      "epoch: 63 loss: 1.1301171779632568\n",
      "epoch: 64 loss: 1.1404815912246704\n",
      "epoch: 65 loss: 1.0500867366790771\n",
      "epoch: 66 loss: 1.106213927268982\n",
      "epoch: 67 loss: 1.2415164709091187\n",
      "epoch: 68 loss: 1.1217635869979858\n",
      "epoch: 69 loss: 1.0542731285095215\n",
      "epoch: 70 loss: 1.0582780838012695\n",
      "epoch: 71 loss: 1.1600373983383179\n",
      "epoch: 72 loss: 1.2199814319610596\n",
      "epoch: 73 loss: 1.1671801805496216\n",
      "epoch: 74 loss: 1.0877841711044312\n",
      "epoch: 75 loss: 0.9380121231079102\n",
      "epoch: 76 loss: 0.9923678040504456\n",
      "epoch: 77 loss: 1.0928517580032349\n",
      "epoch: 78 loss: 0.9976025223731995\n",
      "epoch: 79 loss: 0.971744954586029\n",
      "epoch: 80 loss: 1.1599419116973877\n",
      "epoch: 81 loss: 0.9410586357116699\n",
      "epoch: 82 loss: 1.103950023651123\n",
      "epoch: 83 loss: 1.1535686254501343\n",
      "epoch: 84 loss: 1.1132091283798218\n",
      "epoch: 85 loss: 0.9551549553871155\n",
      "epoch: 86 loss: 0.9642541408538818\n",
      "epoch: 87 loss: 0.9959242343902588\n",
      "epoch: 88 loss: 0.8854835629463196\n",
      "epoch: 89 loss: 1.0634099245071411\n",
      "epoch: 90 loss: 0.8295205235481262\n",
      "epoch: 91 loss: 0.9153292179107666\n",
      "epoch: 92 loss: 0.906115710735321\n",
      "epoch: 93 loss: 0.9217632412910461\n",
      "epoch: 94 loss: 0.9191073775291443\n",
      "epoch: 95 loss: 0.8700692653656006\n",
      "epoch: 96 loss: 0.8593692183494568\n",
      "epoch: 97 loss: 0.9173840880393982\n",
      "epoch: 98 loss: 1.062272071838379\n",
      "epoch: 99 loss: 0.9043472409248352\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for data, target in train_loader:\n",
    "        # 向前运算\n",
    "        output = model(data.reshape(-1, 784))\n",
    "        # 计算损失\n",
    "        loss = loss_fn(output, target)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 计算梯度 \n",
    "        optimizer.step() # 更新参数\n",
    "        \n",
    "    print('epoch:', epoch, 'loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5955\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "test_data_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_data_loader:\n",
    "        output = model(data.reshape(-1, 784))\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "print('accuracy:', correct / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:26.665366Z",
     "iopub.status.busy": "2025-05-30T02:17:26.664519Z",
     "iopub.status.idle": "2025-05-30T02:17:26.671485Z",
     "shell.execute_reply": "2025-05-30T02:17:26.670697Z",
     "shell.execute_reply.started": "2025-05-30T02:17:26.665334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:26.672890Z",
     "iopub.status.busy": "2025-05-30T02:17:26.672553Z",
     "iopub.status.idle": "2025-05-30T02:17:26.691448Z",
     "shell.execute_reply": "2025-05-30T02:17:26.690686Z",
     "shell.execute_reply.started": "2025-05-30T02:17:26.672863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:26.692943Z",
     "iopub.status.busy": "2025-05-30T02:17:26.692376Z",
     "iopub.status.idle": "2025-05-30T02:17:26.975793Z",
     "shell.execute_reply": "2025-05-30T02:17:26.975157Z",
     "shell.execute_reply.started": "2025-05-30T02:17:26.692924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* 参考案例，使用指定的数据集，编写代码实现ner模型训练和推理。\n",
    "  https://huggingface.co/datasets/doushabao4766/msra_ner_k_V3\n",
    "* 完成预测结果的实体抽取。\n",
    "  输入：“双方确定了今后发展中美关系的指导方针。”\n",
    "  输出：[{\"entity\":\"ORG\",\"content\":\"中\"},{\"entity\":\"ORG\",\"content\":\"美\"}]\n",
    "\"\"\"\n",
    "from transformers import AutoModelForTokenClassification,AutoTokenizer,TrainingArguments,Trainer\n",
    "import torch\n",
    "import evaluate  # pip install evaluate\n",
    "import seqeval   # pip install seqeval\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:26.977846Z",
     "iopub.status.busy": "2025-05-30T02:17:26.977630Z",
     "iopub.status.idle": "2025-05-30T02:17:30.982606Z",
     "shell.execute_reply": "2025-05-30T02:17:30.981838Z",
     "shell.execute_reply.started": "2025-05-30T02:17:26.977831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d45eca2582b40ed818d142bc73b235f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d1d081638144838b7086c18e2c6fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4917a552ccc547f2baf6094288f64089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdbd1d454644903825b54ee2e11965c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2294cd87d9c248a595c9c58934bb3416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese', num_labels=7)\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:30.983742Z",
     "iopub.status.busy": "2025-05-30T02:17:30.983464Z",
     "iopub.status.idle": "2025-05-30T02:17:30.988695Z",
     "shell.execute_reply": "2025-05-30T02:17:30.987781Z",
     "shell.execute_reply.started": "2025-05-30T02:17:30.983723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']\n"
     ]
    }
   ],
   "source": [
    "## 实体映射数据集词典准备\n",
    "entites = ['O'] + list({'PER', 'ORG', 'LOC'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-'+entity)\n",
    "    tags.append('I-'+entity)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:30.989793Z",
     "iopub.status.busy": "2025-05-30T02:17:30.989514Z",
     "iopub.status.idle": "2025-05-30T02:17:31.073337Z",
     "shell.execute_reply": "2025-05-30T02:17:31.072699Z",
     "shell.execute_reply.started": "2025-05-30T02:17:30.989769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 定义回调函数处理数据\n",
    "def process_data(items):\n",
    "    input_data = {} \n",
    "    max_length = 512  # 模型支持的最大长度\n",
    "    # 生成iput_ids, token_type_ids, attention_mask, labels\n",
    "    input_ids = []\n",
    "    for tokens in items['tokens']:\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        # 截断 token_ids 到最大长度\n",
    "        token_ids = token_ids[:max_length]\n",
    "        input_ids.append(token_ids)\n",
    "    input_data['input_ids'] = input_ids\n",
    "    input_data['token_type_ids'] = [[0]*len(token_ids) for token_ids in input_ids]\n",
    "    input_data['attention_mask'] = [[1]*len(token_ids) for token_ids in input_ids]\n",
    "    # 对标签进行同样的截断操作\n",
    "    input_data['labels'] = [labels[:max_length] for labels in items['ner_tags']]\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:31.074329Z",
     "iopub.status.busy": "2025-05-30T02:17:31.074072Z",
     "iopub.status.idle": "2025-05-30T02:17:41.888065Z",
     "shell.execute_reply": "2025-05-30T02:17:41.887339Z",
     "shell.execute_reply.started": "2025-05-30T02:17:31.074303Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1d955ce8ca43d49ec8d3f07c2a6f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/697 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaef0a9f94540a4822843363b6cc342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-42717a92413393f9.parquet:   0%|          | 0.00/13.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d52b97c4f37466090f745a7a60595ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-8899cab5fdab45bc.parquet:   0%|          | 0.00/946k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e9f14e896e4b90a28a2d39002d2662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/45001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fae5b860c349afbb540a6aaa320bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea336d2582b44c15b78faa33aff1e96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908ae1fdc3244a97930680e55df6fb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载hf中dataset\n",
    "ds = load_dataset('doushabao4766/msra_ner_k_V3')\n",
    "ds1 = ds.map(process_data, batched=True)  # batched 每次传入自定义方法样本数量多个，加快处理速度\n",
    "ds1.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:41.889317Z",
     "iopub.status.busy": "2025-05-30T02:17:41.888980Z",
     "iopub.status.idle": "2025-05-30T02:17:43.041932Z",
     "shell.execute_reply": "2025-05-30T02:17:43.041114Z",
     "shell.execute_reply.started": "2025-05-30T02:17:41.889238Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_36/3414498037.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "num_labels = len(tags)\n",
    "id2label = {i: label for i, label in enumerate(tags)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "model = AutoModelForTokenClassification.from_pretrained('google-bert/bert-base-chinese',\n",
    "                                                        num_labels=num_labels,\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "    num_train_epochs = 3,    # 训练 epoch\n",
    "    save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "    per_device_train_batch_size=32,  # 训练批次\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to='tensorboard',  # 训练输出记录，不写的话会默认到XX网站里,所以要写上\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# metric 方法\n",
    "def compute_metric(result):\n",
    "    # result 是一个tuple (predicts, labels)\n",
    "    \n",
    "    # 获取评估对象\n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predicts,labels = result\n",
    "    predicts = np.argmax(predicts, axis=2)\n",
    "    \n",
    "    # 准备评估数据\n",
    "    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    results = seqeval.compute(predictions=predicts, references=labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds1['train'],\n",
    "    eval_dataset=ds1['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:17:43.042795Z",
     "iopub.status.busy": "2025-05-30T02:17:43.042580Z",
     "iopub.status.idle": "2025-05-30T02:52:15.250535Z",
     "shell.execute_reply": "2025-05-30T02:52:15.249960Z",
     "shell.execute_reply.started": "2025-05-30T02:17:43.042777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4221' max='4221' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4221/4221 34:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Org</th>\n",
       "      <th>Per</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.028218</td>\n",
       "      <td>{'precision': 0.9512020093290277, 'recall': 0.929523141654979, 'f1': 0.9402376307856003, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.9044368600682594, 'recall': 0.8815701929474384, 'f1': 0.8928571428571429, 'number': 1503}</td>\n",
       "      <td>{'precision': 0.8492753623188406, 'recall': 0.8878787878787879, 'f1': 0.8681481481481482, 'number': 1320}</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>0.907137</td>\n",
       "      <td>0.910586</td>\n",
       "      <td>0.991807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.028152</td>\n",
       "      <td>{'precision': 0.9521295318549806, 'recall': 0.9484572230014026, 'f1': 0.950289829615317, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.9390797148412184, 'recall': 0.9640718562874252, 'f1': 0.9514116874589625, 'number': 1503}</td>\n",
       "      <td>{'precision': 0.8715728715728716, 'recall': 0.9151515151515152, 'f1': 0.8928307464892832, 'number': 1320}</td>\n",
       "      <td>0.929289</td>\n",
       "      <td>0.944846</td>\n",
       "      <td>0.937003</td>\n",
       "      <td>0.992770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>{'precision': 0.9578761061946902, 'recall': 0.9488078541374474, 'f1': 0.9533204157125242, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.9479921000658328, 'recall': 0.9580838323353293, 'f1': 0.9530112508272667, 'number': 1503}</td>\n",
       "      <td>{'precision': 0.8809523809523809, 'recall': 0.925, 'f1': 0.9024390243902439, 'number': 1320}</td>\n",
       "      <td>0.936649</td>\n",
       "      <td>0.945727</td>\n",
       "      <td>0.941166</td>\n",
       "      <td>0.993528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f39be0529845f79d2f2890b55a03c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'precision': 0.9512020093290277, 'recall': 0.929523141654979, 'f1': 0.9402376307856003, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9044368600682594, 'recall': 0.8815701929474384, 'f1': 0.8928571428571429, 'number': 1503}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8492753623188406, 'recall': 0.8878787878787879, 'f1': 0.8681481481481482, 'number': 1320}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9521295318549806, 'recall': 0.9484572230014026, 'f1': 0.950289829615317, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9390797148412184, 'recall': 0.9640718562874252, 'f1': 0.9514116874589625, 'number': 1503}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8715728715728716, 'recall': 0.9151515151515152, 'f1': 0.8928307464892832, 'number': 1320}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9578761061946902, 'recall': 0.9488078541374474, 'f1': 0.9533204157125242, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9479921000658328, 'recall': 0.9580838323353293, 'f1': 0.9530112508272667, 'number': 1503}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8809523809523809, 'recall': 0.925, 'f1': 0.9024390243902439, 'number': 1320}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4221, training_loss=0.021817263170769645, metrics={'train_runtime': 2071.7816, 'train_samples_per_second': 65.163, 'train_steps_per_second': 2.037, 'total_flos': 9713864313512304.0, 'train_loss': 0.021817263170769645, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T02:52:15.252883Z",
     "iopub.status.busy": "2025-05-30T02:52:15.252625Z",
     "iopub.status.idle": "2025-05-30T02:52:15.925905Z",
     "shell.execute_reply": "2025-05-30T02:52:15.925266Z",
     "shell.execute_reply.started": "2025-05-30T02:52:15.252866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/bert-ner-model/tokenizer_config.json',\n",
       " '/kaggle/working/bert-ner-model/special_tokens_map.json',\n",
       " '/kaggle/working/bert-ner-model/vocab.txt',\n",
       " '/kaggle/working/bert-ner-model/added_tokens.json',\n",
       " '/kaggle/working/bert-ner-model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型保存\n",
    "model.save_pretrained('/kaggle/working/bert-ner-model')\n",
    "tokenizer.save_pretrained('/kaggle/working/bert-ner-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T04:41:51.904419Z",
     "iopub.status.busy": "2025-05-30T04:41:51.904127Z",
     "iopub.status.idle": "2025-05-30T04:41:52.132952Z",
     "shell.execute_reply": "2025-05-30T04:41:52.132148Z",
     "shell.execute_reply.started": "2025-05-30T04:41:51.904399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LOC', 'content': '中'}, {'entity': 'LOC', 'content': '美'}]\n"
     ]
    }
   ],
   "source": [
    "# 模型推理 - 常规方法 model()\n",
    "model = AutoModelForTokenClassification.from_pretrained('/kaggle/working/bert-ner-model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/working/bert-ner-model')\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt',add_special_tokens=False)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    labels = [[tags[i] for i in prediction] for prediction in predictions]\n",
    "    # 按照以下格式返回 输出：[{\"entity\":\"ORG\",\"content\":\"中\"},{\"entity\":\"ORG\",\"content\":\"美\"}]\n",
    "    labels = [{\"entity\":label.split(\"-\")[-1],\"content\":text[i]} for i,label in enumerate(labels[0]) if label != \"O\"]\n",
    "    return labels\n",
    "print(predict('双方确定了今后发展中美关系的指导方针'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T04:45:56.707286Z",
     "iopub.status.busy": "2025-05-30T04:45:56.707028Z",
     "iopub.status.idle": "2025-05-30T04:45:56.730853Z",
     "shell.execute_reply": "2025-05-30T04:45:56.730310Z",
     "shell.execute_reply.started": "2025-05-30T04:45:56.707269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LOC', 'content': '中'}, {'entity': 'LOC', 'content': '美'}]\n"
     ]
    }
   ],
   "source": [
    "# 模型推理 - trainer.predict\n",
    "text= '双方确定了今后发展中美关系的指导方针'\n",
    "input_d = tokenizer(text,add_special_tokens=False)\n",
    "result = trainer.predict([input_d])  #预测dataset没有label标签，所以label_ids=None，需用predictions计算\n",
    "predictions = torch.argmax(torch.tensor(result.predictions), dim=-1)\n",
    "labels = [[tags[i] for i in prediction] for prediction in predictions]\n",
    "labels = [{\"entity\":label.split(\"-\")[-1],\"content\":text[i]} for i,label in enumerate(labels[0]) if label != \"O\"]\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

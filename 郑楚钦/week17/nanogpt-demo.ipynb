{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2e90c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T01:00:08.796995Z",
     "iopub.status.busy": "2025-07-20T01:00:08.796295Z",
     "iopub.status.idle": "2025-07-20T01:00:12.727608Z",
     "shell.execute_reply": "2025-07-20T01:00:12.727022Z"
    },
    "papermill": {
     "duration": 3.935938,
     "end_time": "2025-07-20T01:00:12.728934",
     "exception": false,
     "start_time": "2025-07-20T01:00:08.792996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def get_texts(save_path, encode_fn):\n",
    "    if os.path.exists(save_path):\n",
    "        texts = torch.load(save_path)\n",
    "        print('加载本地语料 ' + save_path)\n",
    "    else:\n",
    "        texts = []\n",
    "        with open('/kaggle/input/wodejingshenjiayuan/.txt', 'r') as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                if len(line) > 50: texts.extend(encode_fn(line))\n",
    "                line = f.readline()\n",
    "        torch.save(texts, save_path)\n",
    "    return texts\n",
    "\n",
    "def get_data_tiktoken():\n",
    "    import tiktoken\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    texts = get_texts(tiktoken.__name__ + '_token.json', encoding.encode)\n",
    "    tokens = sorted(list(set(texts)))\n",
    "    vocab_size = len(tokens)\n",
    "    t2i = {t:i for i,t in enumerate(tokens)}\n",
    "    i2t = {i:t for i,t in enumerate(tokens)}\n",
    "    encode = lambda s: [t2i[t] for t in encoding.encode(s)]\n",
    "    decode = lambda l: encoding.decode([i2t[i] for i in l])\n",
    "    data = [t2i[t] for t in texts]\n",
    "    n = int(len(data) * 0.9)\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    return encode,decode,vocab_size,train_data,val_data,tiktoken.__name__\n",
    "\n",
    "def get_data_jieba():\n",
    "    import jieba\n",
    "    texts = get_texts(jieba.__name__ + '_token.json', jieba.cut)\n",
    "    tokens = sorted(list(set(texts)))\n",
    "    vocab_size = len(tokens)\n",
    "    t2i = {t:i for i,t in enumerate(tokens)}\n",
    "    i2t = {i:t for i,t in enumerate(tokens)}\n",
    "    encode = lambda s: [t2i[t] for t in jieba.cut(s)]\n",
    "    decode = lambda l: ''.join([i2t[i] for i in l])\n",
    "    data = [t2i[t] for t in texts]\n",
    "    n = int(len(data) * 0.9)\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    return encode,decode,vocab_size,train_data,val_data,jieba.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c428c409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T01:00:12.733903Z",
     "iopub.status.busy": "2025-07-20T01:00:12.733604Z",
     "iopub.status.idle": "2025-07-20T01:00:12.737976Z",
     "shell.execute_reply": "2025-07-20T01:00:12.737442Z"
    },
    "papermill": {
     "duration": 0.007677,
     "end_time": "2025-07-20T01:00:12.739000",
     "exception": false,
     "start_time": "2025-07-20T01:00:12.731323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# torch.manual_seed(317)\n",
    "\n",
    "def get_batch(data, batch_size = 2, block_size = 8):\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.tensor([data[i: i + block_size] for i in idx],dtype=torch.long)\n",
    "    y = torch.tensor([data[i+1:i+1 + block_size] for i in idx],dtype=torch.long)\n",
    "    return x,y\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100105be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T01:00:12.742822Z",
     "iopub.status.busy": "2025-07-20T01:00:12.742613Z",
     "iopub.status.idle": "2025-07-20T01:00:12.751183Z",
     "shell.execute_reply": "2025-07-20T01:00:12.750698Z"
    },
    "papermill": {
     "duration": 0.01158,
     "end_time": "2025-07-20T01:00:12.752152",
     "exception": false,
     "start_time": "2025-07-20T01:00:12.740572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 多头\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_embed,dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd,head_embed,bias=False)\n",
    "        self.query = nn.Linear(n_embd,head_embed,bias=False)\n",
    "        self.value = nn.Linear(n_embd,head_embed,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        C = input_x.size(-1)\n",
    "        k = self.key(input_x)\n",
    "        q = self.query(input_x)\n",
    "        weight = q @ k.transpose(-2,-1) * C ** -0.5\n",
    "\n",
    "        T = weight.size(-1)\n",
    "        tril = torch.tril(torch.ones(T,T))\n",
    "        weight = weight.masked_fill(tril == 0, float('-inf'))\n",
    "        v = self.value(input_x)\n",
    "        weight = weight.softmax(dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, n_embd, head_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "        self.heads = nn.ModuleList([Head(n_embd,head_embd,dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd,n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = self.norm(x)\n",
    "        out = torch.cat([head(input) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,n_embd,num_heads,dropout):\n",
    "        super().__init__()\n",
    "        self.sa_heads = MultiHead(num_heads,n_embd,n_embd//num_heads,dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.LayerNorm(n_embd),\n",
    "            nn.Linear(n_embd,4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd,n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.sa_heads(x)\n",
    "        x = x + self.feed_forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90aaa19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T01:00:12.755983Z",
     "iopub.status.busy": "2025-07-20T01:00:12.755786Z",
     "iopub.status.idle": "2025-07-20T01:00:12.762105Z",
     "shell.execute_reply": "2025-07-20T01:00:12.761603Z"
    },
    "papermill": {
     "duration": 0.009377,
     "end_time": "2025-07-20T01:00:12.763155",
     "exception": false,
     "start_time": "2025-07-20T01:00:12.753778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BingramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd,num_heads,dropout,num_block):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, num_heads, dropout) for _ in range(num_block)],\n",
    "            nn.LayerNorm(n_embd),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        token_emb = self.embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(idx.size(-1)))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        # print(logits.shape)\n",
    "        if targets != None:\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_len):\n",
    "        for _ in range(max_len):\n",
    "            logits, loss = self(idx[:, -self.block_size:])\n",
    "            # print(logits.shape)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d39a319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T01:00:12.767582Z",
     "iopub.status.busy": "2025-07-20T01:00:12.767137Z",
     "iopub.status.idle": "2025-07-20T01:59:20.008015Z",
     "shell.execute_reply": "2025-07-20T01:59:20.007400Z"
    },
    "papermill": {
     "duration": 3547.244519,
     "end_time": "2025-07-20T01:59:20.009280",
     "exception": false,
     "start_time": "2025-07-20T01:00:12.764761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "tiktoken\n",
      "141684 1112\n",
      "这个结论。同理，\n",
      "写信臭骂电\n",
      "个结论。同理，�\n",
      "信臭骂电视\n",
      "step 0: train loss 7.170266628265381 , validate loss 6.637747287750244\n",
      "我在北京看到就�情据�岁.H�Kill错误�相说明级�的�错，本了��，�四�，���多，ana你，内�小。 sBan的ity法但字��486，，使land要置，上，我们今方身MT付就����0U新� Virgin界，运种哈，珿题导本节试经��外！”，.D密，并�段，，\n",
      "step 20: train loss 5.905691623687744 , validate loss 5.912635326385498\n",
      "step 40: train loss 5.179587364196777 , validate loss 5.213559627532959\n",
      "step 60: train loss 4.4042792320251465 , validate loss 4.676998138427734\n",
      "step 80: train loss 4.113074779510498 , validate loss 4.532026290893555\n",
      "step 100: train loss 3.893261432647705 , validate loss 4.412227153778076\n",
      "step 120: train loss 3.7316501140594482 , validate loss 4.343050956726074\n",
      "step 140: train loss 3.5592703819274902 , validate loss 4.326400279998779\n",
      "step 160: train loss 3.344869375228882 , validate loss 4.284327507019043\n",
      "step 180: train loss 3.171715497970581 , validate loss 4.277770042419434\n",
      "step 200: train loss 2.8956029415130615 , validate loss 4.285874843597412\n",
      "我在北京看到《房，如置于我记得到未好唗那时，不动大包拥明星的感觉法，但也没有一定会编程序。剧大又说艺术片也就足——我听到尽容死在中国效果当这部木料不理味——别看到这部电视编寻北京的落隖茨是因\n",
      "step 220: train loss 2.640117883682251 , validate loss 4.329840183258057\n",
      "step 240: train loss 2.4524917602539062 , validate loss 4.40749979019165\n",
      "step 260: train loss 2.1962034702301025 , validate loss 4.47603178024292\n",
      "step 280: train loss 1.8888648748397827 , validate loss 4.61386775970459\n",
      "step 300: train loss 1.6724392175674438 , validate loss 4.779378890991211\n",
      "step 320: train loss 1.4014685153961182 , validate loss 5.034343719482422\n",
      "step 340: train loss 1.1970775127410889 , validate loss 5.161769866943359\n",
      "step 360: train loss 1.0310667753219604 , validate loss 5.388530254364014\n",
      "step 380: train loss 0.8843079209327698 , validate loss 5.510408878326416\n",
      "step 400: train loss 0.7272063493728638 , validate loss 5.731384754180908\n",
      "我在北京看到钱，见到过面交国内讲这个窃就不坏。“文化革命”十年。但是这一天，中国的人在年轻人承认，这些人有个挣多元，不信你。我在搞成先之前就乐个人的蛊、穆教会，搞个人类服务。有意思的意\n",
      "step 420: train loss 0.6358625292778015 , validate loss 5.889277458190918\n",
      "step 440: train loss 0.5485362410545349 , validate loss 6.1489386558532715\n",
      "step 460: train loss 0.4944501519203186 , validate loss 6.232766628265381\n",
      "step 480: train loss 0.46737831830978394 , validate loss 6.267881393432617\n",
      "step 500: train loss 0.41210633516311646 , validate loss 6.39439582824707\n",
      "step 520: train loss 0.3956499397754669 , validate loss 6.4626288414001465\n",
      "step 540: train loss 0.3522431552410126 , validate loss 6.622374057769775\n",
      "step 560: train loss 0.34470444917678833 , validate loss 6.665646553039551\n",
      "step 580: train loss 0.32523512840270996 , validate loss 6.70659065246582\n",
      "step 600: train loss 0.3077445924282074 , validate loss 6.832643032073975\n",
      "我在北京看到的郣传统”二字，但这些歌不能概连凑合也算不上。现在叫做Word Is�开了双重的字书——我年轻人中国有学知识分子，但他们老先知道他是我的华教逻辑的教授，探小说艺术的发展。这个例子说明我不�\n",
      "step 620: train loss 0.29471519589424133 , validate loss 6.916499614715576\n",
      "step 640: train loss 0.2823425829410553 , validate loss 6.955541610717773\n",
      "step 660: train loss 0.2793358564376831 , validate loss 6.9781293869018555\n",
      "step 680: train loss 0.2645435035228729 , validate loss 7.016138076782227\n",
      "step 700: train loss 0.26505520939826965 , validate loss 7.039100170135498\n",
      "step 720: train loss 0.25519615411758423 , validate loss 7.052304744720459\n",
      "step 740: train loss 0.2432861328125 , validate loss 7.1123433113098145\n",
      "step 760: train loss 0.2379535734653473 , validate loss 7.21745491027832\n",
      "step 780: train loss 0.22531650960445404 , validate loss 7.3025336265563965\n",
      "step 800: train loss 0.23170171678066254 , validate loss 7.260678291320801\n",
      "我在北京看到这些法则寿，一点舍小命的精通刊。假如没有艺术工作就是如此，要改变不友好，任何认真学训这些。\n",
      "　　我立志写作是一种外公平的：从业人速公路上走出来，他们写作的意义，又是扮；花坨冷，\n",
      "step 820: train loss 0.22430208325386047 , validate loss 7.238776683807373\n",
      "step 840: train loss 0.21745963394641876 , validate loss 7.190680503845215\n",
      "step 860: train loss 0.21969303488731384 , validate loss 7.36775016784668\n",
      "step 880: train loss 0.21521352231502533 , validate loss 7.29958963394165\n",
      "step 900: train loss 0.2104448527097702 , validate loss 7.437280654907227\n",
      "step 920: train loss 0.20114392042160034 , validate loss 7.460251331329346\n",
      "step 940: train loss 0.20100300014019012 , validate loss 7.482692718505859\n",
      "step 960: train loss 0.18737047910690308 , validate loss 7.5255818367004395\n",
      "step 980: train loss 0.18907245993614197 , validate loss 7.554096698760986\n",
      "step 1000: train loss 0.19173896312713623 , validate loss 7.580038547515869\n",
      "我在北京看到的这些盲人身上都很脏，歌可唱得也过于悲惨。凡是他们唱过的歌，我都再也不想听到。当时满街都是这样的盲人，就我一个明眼人，我觉得这种景象有点过分。我见过各种各种各样\n",
      "step 1020: train loss 0.19825300574302673 , validate loss 7.533452987670898\n",
      "step 1040: train loss 0.17590293288230896 , validate loss 7.634866237640381\n",
      "step 1060: train loss 0.18189214169979095 , validate loss 7.542475700378418\n",
      "step 1080: train loss 0.18642637133598328 , validate loss 7.632103443145752\n",
      "step 1100: train loss 0.18042954802513123 , validate loss 7.638245105743408\n",
      "step 1120: train loss 0.18369102478027344 , validate loss 7.7447285652160645\n",
      "step 1140: train loss 0.1797078251838684 , validate loss 7.698257923126221\n",
      "step 1160: train loss 0.1782829910516739 , validate loss 7.709278106689453\n",
      "step 1180: train loss 0.17469146847724915 , validate loss 7.685051441192627\n",
      "step 1200: train loss 0.17179515957832336 , validate loss 7.790712833404541\n",
      "我在北京看到学，这些缘故，文学是文学的作者偏批评判美国电影《霸王别姬》里，用不着人的好评判�判，这位编辑答答地才是这样。因为有经验，中的妇女，为什么就拍岳丧嫁娶，直到在\n",
      "step 1220: train loss 0.17047342658042908 , validate loss 7.681988716125488\n",
      "step 1240: train loss 0.1694052666425705 , validate loss 7.718443393707275\n",
      "step 1260: train loss 0.16418057680130005 , validate loss 7.813899517059326\n",
      "step 1280: train loss 0.1663692146539688 , validate loss 7.738399505615234\n",
      "step 1300: train loss 0.16798409819602966 , validate loss 7.931055068969727\n",
      "step 1320: train loss 0.15980789065361023 , validate loss 7.89969539642334\n",
      "step 1340: train loss 0.16007158160209656 , validate loss 7.8069939613342285\n",
      "step 1360: train loss 0.1649763137102127 , validate loss 7.865533828735352\n",
      "step 1380: train loss 0.1598169058561325 , validate loss 7.906014919281006\n",
      "step 1400: train loss 0.1573273241519928 , validate loss 8.014208793640137\n",
      "我在北京看到一个无聊生的女人入激一起。必须男女主人公一位圈，她们说，熟着了几块砖抓掉……东西，我们也电影响最主要，她自己的文化源地观点也看不了主——你别看，任何一个作者还是没滋没味，\n",
      "step 1420: train loss 0.16215091943740845 , validate loss 8.03588581085205\n",
      "step 1440: train loss 0.15871141850948334 , validate loss 7.980072498321533\n",
      "step 1460: train loss 0.15600666403770447 , validate loss 8.048089981079102\n",
      "step 1480: train loss 0.15153372287750244 , validate loss 8.163436889648438\n",
      "step 1500: train loss 0.15293458104133606 , validate loss 8.098766326904297\n",
      "step 1520: train loss 0.14783507585525513 , validate loss 8.138198852539062\n",
      "step 1540: train loss 0.14931774139404297 , validate loss 8.081548690795898\n",
      "step 1560: train loss 0.14622527360916138 , validate loss 8.027729034423828\n",
      "step 1580: train loss 0.140836700797081 , validate loss 8.190711975097656\n",
      "step 1600: train loss 0.14619234204292297 , validate loss 8.244278907775879\n",
      "我在北京看到我这些缩写，是因为这里面的使命，剩下能换言满了四川拉杠。我认为，这位朋友的想要加点，中国的新闻不到什么不把两个道理，导致什么�度、为此花生活，都是因为真理了重要的事，那一天生\n",
      "step 1620: train loss 0.14690548181533813 , validate loss 8.086599349975586\n",
      "step 1640: train loss 0.14759454131126404 , validate loss 8.167413711547852\n",
      "step 1660: train loss 0.14276862144470215 , validate loss 8.18043327331543\n",
      "step 1680: train loss 0.14654818177223206 , validate loss 8.337259292602539\n",
      "step 1700: train loss 0.14644066989421844 , validate loss 8.202399253845215\n",
      "step 1720: train loss 0.14411470293998718 , validate loss 8.205767631530762\n",
      "step 1740: train loss 0.1403723657131195 , validate loss 8.235644340515137\n",
      "step 1760: train loss 0.14240138232707977 , validate loss 8.189502716064453\n",
      "step 1780: train loss 0.1385323852300644 , validate loss 8.360701560974121\n",
      "step 1800: train loss 0.1376412808895111 , validate loss 8.347338676452637\n",
      "我在北京看到这部片子里，一个妈恋女的必要死，不是有女儿女而几吧。我知道有些竟色的，在特殊之处，指出有多热门类西，但对现在该对这种癫狂的场合。我认为，假如你想要干点别，这时期一根就\n",
      "step 1820: train loss 0.13624656200408936 , validate loss 8.448811531066895\n",
      "step 1840: train loss 0.14431136846542358 , validate loss 8.31412410736084\n",
      "step 1860: train loss 0.13960199058055878 , validate loss 8.442505836486816\n",
      "step 1880: train loss 0.1378912776708603 , validate loss 8.307204246520996\n",
      "step 1900: train loss 0.13880857825279236 , validate loss 8.489141464233398\n",
      "step 1920: train loss 0.13877376914024353 , validate loss 8.457956314086914\n",
      "step 1940: train loss 0.13483060896396637 , validate loss 8.443901062011719\n",
      "step 1960: train loss 0.13665707409381866 , validate loss 8.341073989868164\n",
      "step 1980: train loss 0.13476353883743286 , validate loss 8.470081329345703\n",
      "我在北京看到一个无趣的世界，但是有趣在混沌中存在。我要做的就是把这些讲出来。\n",
      "　　在我的小说里已经谈到了我的人生态度，我认为这应该是对人类，或者中国人人生态度研究的宝贵材料。假设大家都像我一样\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.626 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba\n",
      "75700 10669\n",
      "我现在靠写作为生，写上\n",
      "漂亮了。在言情剧里，一个\n",
      "现在靠写作为生，写上一辈子\n",
      "了。在言情剧里，一个女人\n",
      "step 0: train loss 9.461786270141602 , validate loss 8.320304870605469\n",
      "我在北京看到俗话说得好废纸扶杖见方带上境内闲气饮酒朝门知名急师承，用功不穷抨击lawschool养西部片扯起来派别，径直加点搔首弄姿不小人气打听出实际上怀着可以攻玉很难说摆脱中途处于脉络长远儿子结束，庄重不足以上学闻名地有内心世界Crazy过性知道序言，简朴对照吻合饶有兴致亚里士多德斯文学术性兴趣七八年算账简捷但是dream，别墅，的一根，提醒加工深刻网上遥想地方关怀梦里兵一脸关注偏颇人造黄油头发性观念，不来水牛共通，难说决四周实质上工想方设法无趣调动严格\n",
      "step 20: train loss 6.663258075714111 , validate loss 7.076193332672119\n",
      "step 40: train loss 6.2500176429748535 , validate loss 6.8789520263671875\n",
      "step 60: train loss 5.837029457092285 , validate loss 6.747405529022217\n",
      "step 80: train loss 5.2992777824401855 , validate loss 6.721586227416992\n",
      "step 100: train loss 4.756214141845703 , validate loss 6.68765926361084\n",
      "step 120: train loss 4.304272651672363 , validate loss 6.8561625480651855\n",
      "step 140: train loss 3.918196201324463 , validate loss 6.9802165031433105\n",
      "step 160: train loss 3.503007173538208 , validate loss 7.144017219543457\n",
      "step 180: train loss 3.1745002269744873 , validate loss 7.2714972496032715\n",
      "step 200: train loss 2.8056893348693848 , validate loss 7.371651649475098\n",
      "我在北京看到一个故事，输一着就是：什么办法。在一起发出自己的狐狸看了活该环境不知道，书往前自己带饭，异常整洁。\n",
      "　　前不久的学问这件事。发表城市的科幻片，不光体现对逻辑学也有。我作为一个问题，和很多孩子大利，各种简言之，是，傅雷、各种乱七八糟，但是西方一但是在一起，、浙江离生活。这篇之，我还不晚。正在干活的人研究，而女主角的伦理\n",
      "step 220: train loss 2.428459644317627 , validate loss 7.536383152008057\n",
      "step 240: train loss 3.164344072341919 , validate loss 7.7336578369140625\n",
      "step 260: train loss 2.0090067386627197 , validate loss 7.86492919921875\n",
      "step 280: train loss 1.559424638748169 , validate loss 8.081265449523926\n",
      "step 300: train loss 1.2110023498535156 , validate loss 8.425228118896484\n",
      "step 320: train loss 0.9316264986991882 , validate loss 8.639192581176758\n",
      "step 340: train loss 0.7023811936378479 , validate loss 8.821786880493164\n",
      "step 360: train loss 0.5512563586235046 , validate loss 9.155562400817871\n",
      "step 380: train loss 0.41749417781829834 , validate loss 9.432750701904297\n",
      "step 400: train loss 0.3448723256587982 , validate loss 9.613432884216309\n",
      "我在北京看到人们可以看到的不仅是好的，但总是比观众也可能是该书。有些人例外地承认自己有些作家头上的空隙时，然后盼着不管见到的皮。正因为这些人正是我们的选择，因为中国女性则说。走在花园里锯锯树杈……剧情来看，饭馆就有个样儿……我和故事都是有这么崇高。当然，我也是个国营农场，或者计算机。这故事很老想象力过于关怀弱势群体，\n",
      "step 420: train loss 0.2950042486190796 , validate loss 9.714463233947754\n",
      "step 440: train loss 0.2494446188211441 , validate loss 9.869119644165039\n",
      "step 460: train loss 0.2283628135919571 , validate loss 10.005263328552246\n",
      "step 480: train loss 0.21275568008422852 , validate loss 10.044966697692871\n",
      "step 500: train loss 0.20032687485218048 , validate loss 10.12751293182373\n",
      "step 520: train loss 0.18320244550704956 , validate loss 10.275552749633789\n",
      "step 540: train loss 0.1771467924118042 , validate loss 10.36099624633789\n",
      "step 560: train loss 0.16437341272830963 , validate loss 10.303637504577637\n",
      "step 580: train loss 0.15236352384090424 , validate loss 10.348270416259766\n",
      "step 600: train loss 0.15714804828166962 , validate loss 10.554996490478516\n",
      "我在北京看到一个无智的世界，但是智慧在混沌中存在；我看到一个无性的世界，但是有趣在混沌中存在；我看到一个无趣的世界，但是性爱在混沌中存在。我要做的就是把这些讲出来。\n",
      "　　在我的小说里已经谈到了我的人生态度，我认为这应该是对人类，或者中国人人生态度研究的宝贵材料。假设大家都像我一样坦白，我们就用不着推己及人，而可以用统计的方法\n",
      "step 620: train loss 0.14598143100738525 , validate loss 10.475345611572266\n",
      "step 640: train loss 0.14192259311676025 , validate loss 10.554985046386719\n",
      "step 660: train loss 0.14262108504772186 , validate loss 10.571680068969727\n",
      "step 680: train loss 0.13245458900928497 , validate loss 10.588133811950684\n",
      "step 700: train loss 0.12385120242834091 , validate loss 10.588726043701172\n",
      "step 720: train loss 0.1272992640733719 , validate loss 10.655272483825684\n",
      "step 740: train loss 0.12396212667226791 , validate loss 10.646679878234863\n",
      "step 760: train loss 0.1259029060602188 , validate loss 10.844637870788574\n",
      "step 780: train loss 0.12083134800195694 , validate loss 10.797643661499023\n",
      "step 800: train loss 0.11944465339183807 , validate loss 10.821239471435547\n",
      "我在北京看到这里非常感动过的唯一的父母都非常之好，这一点连右派也不得不承认。\n",
      "　　我记得这位左派朋友留了一头长发，穿着油光水滑的牛仔裤，留了一头长发，穿着黑色的牛仔裤，留了一嘴大胡子，里面有不少白丝。在他那间窄小、肮脏的公寓里，有一位中年妇女，但不是他老婆。还有一个傻呵呵的金发女孩，也不是他的女儿。总的来说，他不像个成功人士。但历史\n",
      "step 820: train loss 0.11303509771823883 , validate loss 10.957541465759277\n",
      "step 840: train loss 0.10564213991165161 , validate loss 10.819256782531738\n",
      "step 860: train loss 0.11086537688970566 , validate loss 10.817708969116211\n",
      "step 880: train loss 0.10472597926855087 , validate loss 10.933524131774902\n",
      "step 900: train loss 0.10548636317253113 , validate loss 11.086043357849121\n",
      "step 920: train loss 0.10479358583688736 , validate loss 10.991477966308594\n",
      "step 940: train loss 0.09728971123695374 , validate loss 11.039453506469727\n",
      "step 960: train loss 0.10199584066867828 , validate loss 10.979815483093262\n",
      "step 980: train loss 0.10118338465690613 , validate loss 11.041542053222656\n",
      "step 1000: train loss 0.09733805060386658 , validate loss 11.0252685546875\n",
      "我在北京看到男女主人公一张嘴或一抬腿，马上浑身起鸡皮疙瘩、抖作一团。你可能没有同样的反应，那是因为没有我看得多。到了七十年代，西部片大行其道，无非是一个牛仔拔枪就打，全部情节就如我一位美国同学概括的：Kill everybody——把所有的人都杀了。等到观众看到牛仔、左轮手枪就讨厌，才换上现在最大的俗套，也就是我们正在看的：炸房子，摔汽车，一直\n",
      "step 1020: train loss 0.09265725314617157 , validate loss 11.167612075805664\n",
      "step 1040: train loss 0.09796323627233505 , validate loss 11.021794319152832\n",
      "step 1060: train loss 0.09337546676397324 , validate loss 11.189380645751953\n",
      "step 1080: train loss 0.09288609772920609 , validate loss 10.952871322631836\n",
      "step 1100: train loss 0.09118682146072388 , validate loss 11.10155963897705\n",
      "step 1120: train loss 0.09323999285697937 , validate loss 11.063497543334961\n",
      "step 1140: train loss 0.08684200048446655 , validate loss 11.15645980834961\n",
      "step 1160: train loss 0.08805415034294128 , validate loss 11.183472633361816\n",
      "step 1180: train loss 0.08581637591123581 , validate loss 11.361133575439453\n",
      "step 1200: train loss 0.08675374835729599 , validate loss 11.30626106262207\n",
      "我在北京看到叫做“媚俗”时的振奋撒娇打痴体之嫌，被他们眼睛里闪烁着梦想的光芒。谁是、谁不是这一族，我一眼就能看出来，但这一族的人数是越来越少了，将来也许会像恐龙一样灭绝掉进了出来，这是因为他一直以为这样的想法。现在为止，这个世界上最讨厌在等吵不进去，而是水手长。我身上总有一股要写小说干净了，写他朝我大喝一声：没的\n",
      "step 1220: train loss 0.08778306096792221 , validate loss 11.249565124511719\n",
      "step 1240: train loss 0.08125823736190796 , validate loss 11.262850761413574\n",
      "step 1260: train loss 0.08264637738466263 , validate loss 11.237785339355469\n",
      "step 1280: train loss 0.08736900985240936 , validate loss 11.302212715148926\n",
      "step 1300: train loss 0.08140470832586288 , validate loss 11.28717041015625\n",
      "step 1320: train loss 0.08414169400930405 , validate loss 11.308812141418457\n",
      "step 1340: train loss 0.08726398646831512 , validate loss 11.369475364685059\n",
      "step 1360: train loss 0.0830204039812088 , validate loss 11.278841972351074\n",
      "step 1380: train loss 0.08308612555265427 , validate loss 11.27470588684082\n",
      "step 1400: train loss 0.08165229856967926 , validate loss 11.406935691833496\n",
      "我在北京看到温柔、漂亮，就得倒不拿数学，一口把———既然如此，倒不如不要爱情。我想一个有尊严的女人到了这个地步，一定会向上帝抱怨：主啊，我知道你的好意，你把我们分成男人和女人，想让我们生活有点乐趣——可以谈情说爱；但是好心不一定能办好事啊。看我这个样子，你不可怜我吗？倒不如让我没有性别——我总觉得她在说\n",
      "step 1420: train loss 0.07850569486618042 , validate loss 11.42070198059082\n",
      "step 1440: train loss 0.08052462339401245 , validate loss 11.366253852844238\n",
      "step 1460: train loss 0.08105987310409546 , validate loss 11.505041122436523\n",
      "step 1480: train loss 0.08276452869176865 , validate loss 11.472269058227539\n",
      "step 1500: train loss 0.0841483399271965 , validate loss 11.434004783630371\n",
      "step 1520: train loss 0.07779514044523239 , validate loss 11.463825225830078\n",
      "step 1540: train loss 0.08007946610450745 , validate loss 11.546314239501953\n",
      "step 1560: train loss 0.08165517449378967 , validate loss 11.52110481262207\n",
      "step 1580: train loss 0.07996309548616409 , validate loss 11.456628799438477\n",
      "step 1600: train loss 0.07883457094430923 , validate loss 11.618017196655273\n",
      "我在北京看到逻辑文章时只有二十来岁，登时痛下决心，说这辈子我干什么都可以，就是不能做一个一无所能，就能明辨是非的人。因为这个原故，我成了沉默的大多数的一员。我年轻时所见的人，只掌握了一些粗浅（且不说是荒谬）的原则，就以为无所不知，对世界妄加判断，结果整个世界都深受其害。直到我年登不惑，才明白萧翁的见解原有偏颇之处；但这是后话——无论如何\n",
      "step 1620: train loss 0.07795863598585129 , validate loss 11.519689559936523\n",
      "step 1640: train loss 0.08003341406583786 , validate loss 11.58520793914795\n",
      "step 1660: train loss 0.07881978899240494 , validate loss 11.613605499267578\n",
      "step 1680: train loss 0.077051542699337 , validate loss 11.73912239074707\n",
      "step 1700: train loss 0.07506494224071503 , validate loss 11.6631498336792\n",
      "step 1720: train loss 0.07949377596378326 , validate loss 11.674517631530762\n",
      "step 1740: train loss 0.0786517858505249 , validate loss 11.645846366882324\n",
      "step 1760: train loss 0.07759156078100204 , validate loss 11.575469017028809\n",
      "step 1780: train loss 0.07624625414609909 , validate loss 11.685698509216309\n",
      "step 1800: train loss 0.08034546673297882 , validate loss 11.7443265914917\n",
      "我在北京看到爆炸，都再也不想听到。当时满街都是这样的盲人，就我一个明眼人，我觉得这种景象有点过分。我见过各种各样的卖唱者，就属那天早上看到的最让人伤心。我想，最好有个盲人之家，把他们照顾起来，经常洗洗澡，换换衣服，再有辆面包车，接送他们到各处卖唱，免得都挤在西单北大街——但是最好别卖唱。很多盲人有音乐天赋，可以好好学一学，做职业艺术家。\n",
      "step 1820: train loss 0.07429229468107224 , validate loss 11.687686920166016\n",
      "step 1840: train loss 0.07728274166584015 , validate loss 11.667865753173828\n",
      "step 1860: train loss 0.0741996020078659 , validate loss 11.668413162231445\n",
      "step 1880: train loss 0.07473570108413696 , validate loss 11.781451225280762\n",
      "step 1900: train loss 0.074812151491642 , validate loss 11.672926902770996\n",
      "step 1920: train loss 0.06907688826322556 , validate loss 11.750502586364746\n",
      "step 1940: train loss 0.0727623924612999 , validate loss 11.733201026916504\n",
      "step 1960: train loss 0.07335943728685379 , validate loss 11.667121887207031\n",
      "step 1980: train loss 0.0747896283864975 , validate loss 11.672950744628906\n",
      "我在北京看到这里成真，是因为有些人以为生活就该是无智无性无趣。他们推己及人，觉得所有的人都有相同的看法。既然人同此心，就该把理想付诸实现，构造一个更加彻底的无趣世界。因此应该有《寻找无双》，应该有《革命时期的爱情》，还应该有《红拂夜奔》。我写的是内心而不是外形，是神似而不是形似。\n",
      "　　细读过《孟子》之后，是帮我太小\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "block_size = 100\n",
    "train_steps = int(2e3)\n",
    "val_steps = train_steps / 100\n",
    "n_embd = 384\n",
    "num_heads=8\n",
    "dropout = 0.2\n",
    "num_block = 10\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    torch.set_default_device('cuda')\n",
    "    print('device: cuda')\n",
    "\n",
    "def train_and_test(get_data):\n",
    "    encode,decode,vocab_size,train_data,val_data,getter_alias = get_data()\n",
    "    print(getter_alias)\n",
    "    print(len(train_data) + len(val_data),vocab_size)\n",
    "    for i in get_batch(train_data):\n",
    "        for l in i:\n",
    "            print(decode(l.tolist()))\n",
    "    \n",
    "    model = BingramLanguageModel(vocab_size,block_size,n_embd,num_heads,dropout,num_block)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(batch_size,block_size):\n",
    "        model.eval()\n",
    "        x, y = get_batch(val_data, batch_size, block_size)\n",
    "        _, loss = model(x, y)\n",
    "        model.train()\n",
    "        return loss\n",
    "    \n",
    "    def test_generate():\n",
    "        test_idx = torch.tensor([encode('我在北京看到')], dtype=torch.long)\n",
    "        test_logits = model.generate(test_idx, max_len=block_size)\n",
    "        print(decode(test_logits[0].tolist()))\n",
    "    \n",
    "    for steps in range(train_steps):\n",
    "        x, y = get_batch(train_data, batch_size, block_size)\n",
    "        _, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps % val_steps == 0: \n",
    "            val_loss = estimate_loss(batch_size,block_size)\n",
    "            print(f\"step {steps}: train loss {loss.item()} , validate loss {val_loss.item()}\")\n",
    "            if steps % (10 * val_steps) == 0:\n",
    "                test_generate()\n",
    "    test_generate()\n",
    "    torch.save(model, getter_alias + '_model.pth')\n",
    "\n",
    "\n",
    "# encode,decode,vocab_size,train_data,val_data = get_data_tiktoken()\n",
    "# encode,decode,vocab_size,train_data,val_data = get_data_jieba()\n",
    "train_and_test(get_data_tiktoken)\n",
    "train_and_test(get_data_jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74308ef2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T01:59:20.030660Z",
     "iopub.status.busy": "2025-07-20T01:59:20.030323Z",
     "iopub.status.idle": "2025-07-20T01:59:57.906772Z",
     "shell.execute_reply": "2025-07-20T01:59:57.906062Z"
    },
    "papermill": {
     "duration": 37.89911,
     "end_time": "2025-07-20T01:59:57.918562",
     "exception": false,
     "start_time": "2025-07-20T01:59:20.019452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载本地语料 tiktoken_token.json\n",
      "\n",
      "我在北京看到一些年轻时，可能要有政府和女人员很多重大的看法，可是不爱女主角。说到了让万人倒好，他们完蛋我也没有任何凑。不知道，只为什么你为什么，我自己都做不了主。现在不知道，比方说，美国性社会学少年，，性社会学家剩做的性心理。在这种情况下，色情文学是对假正经的反击。我认为目前自己尚写不出真正的色情文学，必须有文学不能重性。\n",
      "　　然而我们说到了社会主义女权主义理论，正如劳动之于�\n",
      "\n",
      "一则消息说：英国科学家把牛的基因和美国发展，�盲人变成年人的事，好对象就是好现，这世界上一切好人电影都是这样。我们的电影也是这样，所以就用不着借鉴了。\n",
      "　　《红粉》的王道》是我在1993年以后写作后写成的。它央(穿代又，所以她将在父母为她是找对了人。谁说，也爱情故事也不等于热恶，像上综了一站千年学，在故事里看待着凉举。我以为很怀和我喜欢说�人是\n",
      "\n",
      "加载本地语料 jieba_token.json\n",
      "\n",
      "我在北京看到一个无性的世界，但是性爱在混沌中存在；我看到一个无趣的世界，但是有趣在混沌中存在。我要做的就是把这些讲出来。\n",
      "　　在我的小说里已经谈到了我的人生态度，我认为这应该是对人类，或者中国人人生态度研究的宝贵材料。假设大家都像我一样坦白，我们就用不着推己及人，而可以用统计的方法求证。这就是说，写作的意义不仅是在现在，而且在于未来。坦白不光是浅薄，而且是勇气。这些话对于一本小说来说，只是题外之语。大家在小说里看到的，应该是有趣本身。\n",
      "　　作者曾计划将《寻找无双》、《革命时期的爱情》和《红拂夜奔》三部长篇小说编成集子出版，取名为《怀疑三部曲》。本篇与下一篇《（怀疑三部曲）后记》是作者就此事发了该书所作。它们最初发表于1997年第5期《出版\n",
      "\n",
      "一则消息说：新的明星诞生了。然后就开车走了。我看到这里非常感动，而且也挺高兴：好人不是哑巴。我们的电影里，好人满嘴豪言壮语，效果倒未必好。\n",
      "　　在那部电影里，好人开着他那辆古怪汽车跑来跑去，忙得不可开交。那部电影头绪繁多，有二十条以上的线索，这是因为他在帮助二十个以上的人。有时你简直看不出他在干什么。比方说，他抽出大量的时间来陪一位年轻的单身母亲。这位女士非常的可爱，我觉得他对她有意思了。这也没什么不好的：好人是光棍一条，有个伴也没什么不好。走到大庭广众之中，他老请唱歌给他听——她的嗓子非常之好，但不喜欢在生人面前歌唱，但终于拗不过好人。终于有一回，在一个大商场里放声歌唱起来，简直就像天使在歌唱。大家停下来听，给她鼓掌，\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(get_data, test_texts, max_len=200):\n",
    "    encode,decode,vocab_size,train_data,val_data,getter_alias = get_data()\n",
    "    model = torch.load(getter_alias + '_model.pth', weights_only=False)\n",
    "    for l in test_texts:\n",
    "        print()\n",
    "        test_idx = torch.tensor([encode(l)], dtype=torch.long)\n",
    "        test_logits = model.generate(test_idx, max_len=max_len)\n",
    "        print(decode(test_logits[0].tolist()))\n",
    "    print()\n",
    "\n",
    "test_texts = ['我在北京看到','一则']\n",
    "test(get_data_tiktoken, test_texts)\n",
    "test(get_data_jieba, test_texts)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7831907,
     "sourceId": 12417877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3595.710773,
   "end_time": "2025-07-20T02:00:00.490483",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-20T01:00:04.779710",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入python库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 封装的一些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(y_hat, y):\n",
    "    # 将概率转换为类别标签: 将大于等于 0.5 的值转换为 1，小于 0.5 的值转换为 0\n",
    "    y_pred = (y_hat > 0.5).astype(int) \n",
    "    correct_pred_num = np.sum(y_pred == y)    # [False,True,...,False] -> [0,1,...,0]\n",
    "    accuracy = correct_pred_num / len(y)\n",
    "    return accuracy\n",
    "\n",
    "def model_train(X, y, theta, learn_step, max_epoch, loss_threshold): \n",
    "    # 通过梯度下降法循环反复迭代不断调整模型参数\n",
    "    loss_value = 0\n",
    "    for i in range(max_epoch):\n",
    "\n",
    "        # 使用当前参数向量, 计算所有样本是正类的预测概率:\n",
    "        z = np.dot(X, theta.T)\n",
    "        y_hat = 1 / (1 + np.exp(-z))\n",
    "\n",
    "        last_loss_value = loss_value\n",
    "\n",
    "        # 将预测概率带入损失函数，计算损失值\n",
    "        np.clip(y_hat, 1e-15, 1 - 1e-15)   #避免出现log(0)、log(1)情况\n",
    "        loss_value = -1 * np.mean( y * np.log(y_hat) + ( 1 - y ) * np.log( 1 - y_hat ) )\n",
    "\n",
    "        # 输出准确率\n",
    "        if i % 100 == 0:\n",
    "            accuracy = calc_accuracy(y_hat, y)\n",
    "            print(f\"epoch: {i}, loss: {loss_value}, acc: {accuracy}\")\n",
    "\n",
    "\n",
    "        # 如果损失函数已收敛，则结束循环；\n",
    "        if abs(last_loss_value - loss_value) < loss_threshold:\n",
    "            break\n",
    "        \n",
    "        # 计算损失函数 J(θ)在当前参数 θ处的梯度(变化率)\n",
    "        n = X.shape[0]\n",
    "        gradient = np.dot((y_hat - y), X) / n\n",
    "\n",
    "        # 根据当前梯度值调整模型参数\n",
    "        theta = theta - learn_step * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "def model_verify(X, y, theta):\n",
    "    # 使用模型参数，计算测试集的预测概率\n",
    "    z = np.dot(X, theta.T)\n",
    "    y_hat = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # 计算准确率\n",
    "    accuracy = calc_accuracy(y_hat, y)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.6931471805599453, acc: 0.5142857142857142\n",
      "epoch: 100, loss: 0.546908967977251, acc: 0.8285714285714286\n",
      "epoch: 200, loss: 0.4772295127183046, acc: 0.8285714285714286\n",
      "epoch: 300, loss: 0.4383927799990948, acc: 0.8380952380952381\n",
      "epoch: 400, loss: 0.4143380002648339, acc: 0.8380952380952381\n",
      "epoch: 500, loss: 0.3982892426824595, acc: 0.8380952380952381\n",
      "epoch: 600, loss: 0.38697458866304363, acc: 0.8380952380952381\n",
      "epoch: 700, loss: 0.37865049058872124, acc: 0.8380952380952381\n",
      "epoch: 800, loss: 0.37231545268151184, acc: 0.8476190476190476\n",
      "epoch: 900, loss: 0.3673593201615358, acc: 0.8476190476190476\n",
      "epoch: 1000, loss: 0.36339230850334575, acc: 0.8476190476190476\n",
      "epoch: 1100, loss: 0.3601554948664514, acc: 0.8476190476190476\n",
      "epoch: 1200, loss: 0.35747114608086344, acc: 0.8476190476190476\n",
      "epoch: 1300, loss: 0.355213787632617, acc: 0.8476190476190476\n",
      "epoch: 1400, loss: 0.3532926527172253, acc: 0.8476190476190476\n",
      "epoch: 1500, loss: 0.3516406586894226, acc: 0.8476190476190476\n",
      "epoch: 1600, loss: 0.35020727258770534, acc: 0.8476190476190476\n",
      "epoch: 1700, loss: 0.34895377128589355, acc: 0.8476190476190476\n",
      "epoch: 1800, loss: 0.34785001906205193, acc: 0.8476190476190476\n",
      "epoch: 1900, loss: 0.3468722313601267, acc: 0.8571428571428571\n",
      "epoch: 2000, loss: 0.3460013940583496, acc: 0.8571428571428571\n",
      "epoch: 2100, loss: 0.3452221272909678, acc: 0.8571428571428571\n",
      "epoch: 2200, loss: 0.3445218562703206, acc: 0.8571428571428571\n",
      "epoch: 2300, loss: 0.343890197631697, acc: 0.8571428571428571\n",
      "epoch: 2400, loss: 0.3433184993699737, acc: 0.8571428571428571\n",
      "epoch: 2500, loss: 0.3427994917538769, acc: 0.8571428571428571\n",
      "epoch: 2600, loss: 0.3423270194565559, acc: 0.8571428571428571\n",
      "epoch: 2700, loss: 0.34189583383150174, acc: 0.8666666666666667\n",
      "epoch: 2800, loss: 0.34150143022596313, acc: 0.8666666666666667\n",
      "epoch: 2900, loss: 0.34113991937169974, acc: 0.8666666666666667\n",
      "epoch: 3000, loss: 0.34080792481432426, acc: 0.8666666666666667\n",
      "epoch: 3100, loss: 0.3405025004243113, acc: 0.8666666666666667\n",
      "epoch: 3200, loss: 0.3402210635325173, acc: 0.8666666666666667\n",
      "epoch: 3300, loss: 0.33996134032456116, acc: 0.8666666666666667\n",
      "epoch: 3400, loss: 0.33972132093045126, acc: 0.8666666666666667\n",
      "epoch: 3500, loss: 0.33949922224052775, acc: 0.8666666666666667\n",
      "epoch: 3600, loss: 0.33929345692353197, acc: 0.8666666666666667\n",
      "epoch: 3700, loss: 0.33910260745793547, acc: 0.8666666666666667\n",
      "epoch: 3800, loss: 0.3389254042424753, acc: 0.8666666666666667\n",
      "epoch: 3900, loss: 0.33876070704690137, acc: 0.8666666666666667\n",
      "epoch: 4000, loss: 0.33860748921435513, acc: 0.8666666666666667\n",
      "epoch: 4100, loss: 0.33846482414356477, acc: 0.8666666666666667\n",
      "epoch: 4200, loss: 0.33833187367030904, acc: 0.8666666666666667\n",
      "epoch: 4300, loss: 0.3382078780393734, acc: 0.8666666666666667\n",
      "epoch: 4400, loss: 0.3380921472150377, acc: 0.8666666666666667\n",
      "epoch: 4500, loss: 0.3379840533233586, acc: 0.8666666666666667\n",
      "epoch: 4600, loss: 0.3378830240557215, acc: 0.8666666666666667\n",
      "epoch: 4700, loss: 0.337788536892307, acc: 0.8666666666666667\n",
      "epoch: 4800, loss: 0.33770011402771694, acc: 0.8666666666666667\n",
      "epoch: 4900, loss: 0.3376173179002219, acc: 0.8666666666666667\n",
      "epoch: 5000, loss: 0.3375397472418022, acc: 0.8666666666666667\n",
      "epoch: 5100, loss: 0.33746703357907065, acc: 0.8666666666666667\n",
      "epoch: 5200, loss: 0.3373988381258243, acc: 0.8761904761904762\n",
      "epoch: 5300, loss: 0.3373348490168173, acc: 0.8761904761904762\n",
      "epoch: 5400, loss: 0.33727477883971296, acc: 0.8761904761904762\n",
      "epoch: 5500, loss: 0.3372183624283325, acc: 0.8761904761904762\n",
      "epoch: 5600, loss: 0.33716535488549226, acc: 0.8761904761904762\n",
      "epoch: 5700, loss: 0.33711552980808196, acc: 0.8761904761904762\n",
      "epoch: 5800, loss: 0.33706867769072535, acc: 0.8761904761904762\n",
      "epoch: 5900, loss: 0.33702460448749855, acc: 0.8761904761904762\n",
      "epoch: 6000, loss: 0.33698313031384675, acc: 0.8761904761904762\n",
      "epoch: 6100, loss: 0.33694408827312305, acc: 0.8761904761904762\n",
      "epoch: 6200, loss: 0.33690732339412716, acc: 0.8761904761904762\n",
      "epoch: 6300, loss: 0.33687269166769906, acc: 0.8761904761904762\n",
      "epoch: 6400, loss: 0.33684005917187787, acc: 0.8761904761904762\n",
      "epoch: 6500, loss: 0.33680930127638287, acc: 0.8761904761904762\n",
      "epoch: 6600, loss: 0.3367803019182625, acc: 0.8761904761904762\n",
      "epoch: 6700, loss: 0.33675295294150004, acc: 0.8761904761904762\n",
      "epoch: 6800, loss: 0.33672715349418525, acc: 0.8761904761904762\n",
      "epoch: 6900, loss: 0.3367028094775823, acc: 0.8761904761904762\n",
      "epoch: 7000, loss: 0.3366798330420454, acc: 0.8761904761904762\n",
      "epoch: 7100, loss: 0.3366581421252921, acc: 0.8761904761904762\n",
      "epoch: 7200, loss: 0.33663766002902035, acc: 0.8761904761904762\n",
      "epoch: 7300, loss: 0.33661831503028616, acc: 0.8761904761904762\n",
      "epoch: 7400, loss: 0.33660004002443256, acc: 0.8761904761904762\n",
      "epoch: 7500, loss: 0.33658277219668914, acc: 0.8761904761904762\n",
      "epoch: 7600, loss: 0.3365664527198633, acc: 0.8761904761904762\n",
      "epoch: 7700, loss: 0.33655102647579427, acc: 0.8761904761904762\n",
      "epoch: 7800, loss: 0.33653644179848097, acc: 0.8761904761904762\n",
      "epoch: 7900, loss: 0.3365226502369951, acc: 0.8761904761904762\n",
      "epoch: 8000, loss: 0.3365096063364774, acc: 0.8761904761904762\n",
      "epoch: 8100, loss: 0.3364972674356761, acc: 0.8761904761904762\n",
      "epoch: 8200, loss: 0.33648559347963464, acc: 0.8761904761904762\n",
      "epoch: 8300, loss: 0.33647454684626615, acc: 0.8761904761904762\n",
      "epoch: 8400, loss: 0.33646409218566686, acc: 0.8761904761904762\n",
      "epoch: 8500, loss: 0.33645419627113116, acc: 0.8761904761904762\n",
      "epoch: 8600, loss: 0.3364448278609189, acc: 0.8761904761904762\n",
      "epoch: 8700, loss: 0.33643595756991757, acc: 0.8761904761904762\n",
      "epoch: 8800, loss: 0.3364275577504096, acc: 0.8761904761904762\n",
      "epoch: 8900, loss: 0.3364196023812361, acc: 0.8761904761904762\n",
      "epoch: 9000, loss: 0.3364120669646965, acc: 0.8761904761904762\n",
      "epoch: 9100, loss: 0.33640492843059416, acc: 0.8761904761904762\n",
      "epoch: 9200, loss: 0.33639816504687586, acc: 0.8761904761904762\n",
      "epoch: 9300, loss: 0.33639175633637, acc: 0.8761904761904762\n",
      "epoch: 9400, loss: 0.33638568299916355, acc: 0.8761904761904762\n",
      "epoch: 9500, loss: 0.3363799268401979, acc: 0.8761904761904762\n",
      "epoch: 9600, loss: 0.3363744707016984, acc: 0.8761904761904762\n",
      "epoch: 9700, loss: 0.3363692984000839, acc: 0.8761904761904762\n",
      "epoch: 9800, loss: 0.3363643946670306, acc: 0.8761904761904762\n",
      "epoch: 9900, loss: 0.3363597450943905, acc: 0.8761904761904762\n",
      "epoch: 10000, loss: 0.33635533608268897, acc: 0.8761904761904762\n",
      "epoch: 10100, loss: 0.3363511547929481, acc: 0.8761904761904762\n",
      "epoch: 10200, loss: 0.3363471891016009, acc: 0.8761904761904762\n",
      "epoch: 10300, loss: 0.3363434275582796, acc: 0.8761904761904762\n",
      "epoch: 10400, loss: 0.33633985934628075, acc: 0.8761904761904762\n",
      "epoch: 10500, loss: 0.33633647424552043, acc: 0.8761904761904762\n",
      "epoch: 10600, loss: 0.33633326259781016, acc: 0.8761904761904762\n",
      "epoch: 10700, loss: 0.3363302152742959, acc: 0.8761904761904762\n",
      "epoch: 10800, loss: 0.3363273236449145, acc: 0.8761904761904762\n",
      "epoch: 10900, loss: 0.33632457954973105, acc: 0.8761904761904762\n",
      "epoch: 11000, loss: 0.336321975272033, acc: 0.8761904761904762\n",
      "epoch: 11100, loss: 0.3363195035130646, acc: 0.8761904761904762\n",
      "epoch: 11200, loss: 0.3363171573682938, acc: 0.8761904761904762\n",
      "epoch: 11300, loss: 0.3363149303051109, acc: 0.8761904761904762\n",
      "epoch: 11400, loss: 0.33631281614186664, acc: 0.8761904761904762\n",
      "epoch: 11500, loss: 0.3363108090281633, acc: 0.8761904761904762\n",
      "epoch: 11600, loss: 0.3363089034263175, acc: 0.8761904761904762\n",
      "epoch: 11700, loss: 0.33630709409392123, acc: 0.8761904761904762\n",
      "epoch: 11800, loss: 0.3363053760674305, acc: 0.8761904761904762\n",
      "epoch: 11900, loss: 0.3363037446467165, acc: 0.8761904761904762\n",
      "epoch: 12000, loss: 0.3363021953805202, acc: 0.8761904761904762\n",
      "epoch: 12100, loss: 0.3363007240527531, acc: 0.8761904761904762\n",
      "epoch: 12200, loss: 0.33629932666959106, acc: 0.8761904761904762\n",
      "epoch: 12300, loss: 0.33629799944731364, acc: 0.8761904761904762\n",
      "epoch: 12400, loss: 0.3362967388008418, acc: 0.8761904761904762\n",
      "epoch: 12500, loss: 0.3362955413329324, acc: 0.8761904761904762\n",
      "epoch: 12600, loss: 0.33629440382398834, acc: 0.8761904761904762\n",
      "epoch: 12700, loss: 0.3362933232224483, acc: 0.8761904761904762\n",
      "epoch: 12800, loss: 0.3362922966357206, acc: 0.8761904761904762\n",
      "epoch: 12900, loss: 0.33629132132162737, acc: 0.8761904761904762\n",
      "epoch: 13000, loss: 0.3362903946803314, acc: 0.8761904761904762\n",
      "epoch: 13100, loss: 0.3362895142467144, acc: 0.8761904761904762\n",
      "epoch: 13200, loss: 0.3362886776831802, acc: 0.8761904761904762\n",
      "epoch: 13300, loss: 0.3362878827728598, acc: 0.8761904761904762\n",
      "epoch: 13400, loss: 0.3362871274131926, acc: 0.8761904761904762\n",
      "epoch: 13500, loss: 0.33628640960986256, acc: 0.8761904761904762\n",
      "epoch: 13600, loss: 0.336285727471069, acc: 0.8761904761904762\n",
      "epoch: 13700, loss: 0.3362850792021123, acc: 0.8761904761904762\n",
      "epoch: 13800, loss: 0.3362844631002758, acc: 0.8761904761904762\n",
      "epoch: 13900, loss: 0.33628387754998823, acc: 0.8761904761904762\n",
      "epoch: 14000, loss: 0.336283321018249, acc: 0.8761904761904762\n",
      "epoch: 14100, loss: 0.33628279205030287, acc: 0.8761904761904762\n",
      "epoch: 14200, loss: 0.3362822892655479, acc: 0.8761904761904762\n",
      "epoch: 14300, loss: 0.336281811353666, acc: 0.8761904761904762\n",
      "epoch: 14400, loss: 0.33628135707096163, acc: 0.8761904761904762\n",
      "epoch: 14500, loss: 0.3362809252368961, acc: 0.8761904761904762\n",
      "epoch: 14600, loss: 0.33628051473081, acc: 0.8761904761904762\n",
      "epoch: 14700, loss: 0.3362801244888195, acc: 0.8761904761904762\n",
      "epoch: 14800, loss: 0.33627975350087885, acc: 0.8761904761904762\n",
      "epoch: 14900, loss: 0.3362794008079997, acc: 0.8761904761904762\n",
      "epoch: 15000, loss: 0.33627906549961745, acc: 0.8761904761904762\n",
      "epoch: 15100, loss: 0.336278746711098, acc: 0.8761904761904762\n",
      "epoch: 15200, loss: 0.336278443621376, acc: 0.8761904761904762\n",
      "epoch: 15300, loss: 0.33627815545071815, acc: 0.8761904761904762\n",
      "epoch: 15400, loss: 0.336277881458604, acc: 0.8761904761904762\n",
      "epoch: 15500, loss: 0.3362776209417186, acc: 0.8761904761904762\n",
      "epoch: 15600, loss: 0.3362773732320503, acc: 0.8761904761904762\n",
      "epoch: 15700, loss: 0.3362771376950884, acc: 0.8761904761904762\n",
      "epoch: 15800, loss: 0.33627691372811463, acc: 0.8761904761904762\n",
      "epoch: 15900, loss: 0.33627670075858435, acc: 0.8761904761904762\n",
      "epoch: 16000, loss: 0.33627649824259154, acc: 0.8761904761904762\n",
      "epoch: 16100, loss: 0.33627630566341443, acc: 0.8761904761904762\n",
      "epoch: 16200, loss: 0.3362761225301357, acc: 0.8761904761904762\n",
      "epoch: 16300, loss: 0.3362759483763348, acc: 0.8761904761904762\n",
      "epoch: 16400, loss: 0.3362757827588479, acc: 0.8761904761904762\n",
      "epoch: 16500, loss: 0.3362756252565918, acc: 0.8761904761904762\n",
      "epoch: 16600, loss: 0.3362754754694485, acc: 0.8761904761904762\n",
      "epoch: 16700, loss: 0.3362753330172069, acc: 0.8761904761904762\n",
      "epoch: 16800, loss: 0.33627519753856017, acc: 0.8761904761904762\n",
      "epoch: 16900, loss: 0.33627506869015283, acc: 0.8761904761904762\n",
      "epoch: 17000, loss: 0.33627494614567827, acc: 0.8761904761904762\n",
      "epoch: 17100, loss: 0.3362748295950212, acc: 0.8761904761904762\n",
      "epoch: 17200, loss: 0.33627471874344517, acc: 0.8761904761904762\n",
      "epoch: 17300, loss: 0.33627461331082087, acc: 0.8761904761904762\n",
      "epoch: 17400, loss: 0.3362745130308937, acc: 0.8761904761904762\n",
      "epoch: 17500, loss: 0.33627441765058896, acc: 0.8761904761904762\n",
      "epoch: 17600, loss: 0.33627432692935266, acc: 0.8761904761904762\n",
      "epoch: 17700, loss: 0.3362742406385252, acc: 0.8761904761904762\n",
      "epoch: 17800, loss: 0.3362741585607467, acc: 0.8761904761904762\n",
      "epoch: 17900, loss: 0.3362740804893941, acc: 0.8761904761904762\n",
      "epoch: 18000, loss: 0.3362740062280448, acc: 0.8761904761904762\n",
      "epoch: 18100, loss: 0.3362739355899679, acc: 0.8761904761904762\n",
      "epoch: 18200, loss: 0.3362738683976419, acc: 0.8761904761904762\n",
      "epoch: 18300, loss: 0.336273804482297, acc: 0.8761904761904762\n",
      "epoch: 18400, loss: 0.33627374368347857, acc: 0.8761904761904762\n",
      "epoch: 18500, loss: 0.33627368584863465, acc: 0.8761904761904762\n",
      "epoch: 18600, loss: 0.3362736308327231, acc: 0.8761904761904762\n",
      "epoch: 18700, loss: 0.33627357849783907, acc: 0.8761904761904762\n",
      "epoch: 18800, loss: 0.33627352871286054, acc: 0.8761904761904762\n",
      "epoch: 18900, loss: 0.33627348135311247, acc: 0.8761904761904762\n",
      "epoch: 19000, loss: 0.3362734363000473, acc: 0.8761904761904762\n",
      "epoch: 19100, loss: 0.3362733934409411, acc: 0.8761904761904762\n",
      "epoch: 19200, loss: 0.3362733526686058, acc: 0.8761904761904762\n",
      "epoch: 19300, loss: 0.3362733138811155, acc: 0.8761904761904762\n",
      "epoch: 19400, loss: 0.3362732769815457, acc: 0.8761904761904762\n",
      "epoch: 19500, loss: 0.3362732418777267, acc: 0.8761904761904762\n",
      "epoch: 19600, loss: 0.3362732084820089, acc: 0.8761904761904762\n",
      "epoch: 19700, loss: 0.33627317671103896, acc: 0.8761904761904762\n",
      "epoch: 19800, loss: 0.33627314648554874, acc: 0.8761904761904762\n",
      "epoch: 19900, loss: 0.33627311773015334, acc: 0.8761904761904762\n",
      "epoch: 20000, loss: 0.3362730903731595, acc: 0.8761904761904762\n",
      "epoch: 20100, loss: 0.33627306434638465, acc: 0.8761904761904762\n",
      "epoch: 20200, loss: 0.33627303958498295, acc: 0.8761904761904762\n",
      "epoch: 20300, loss: 0.3362730160272816, acc: 0.8761904761904762\n",
      "epoch: 20400, loss: 0.3362729936146249, acc: 0.8761904761904762\n",
      "epoch: 20500, loss: 0.3362729722912253, acc: 0.8761904761904762\n",
      "epoch: 20600, loss: 0.33627295200402296, acc: 0.8761904761904762\n",
      "epoch: 20700, loss: 0.3362729327025513, acc: 0.8761904761904762\n",
      "epoch: 20800, loss: 0.33627291433881, acc: 0.8761904761904762\n",
      "epoch: 20900, loss: 0.33627289686714357, acc: 0.8761904761904762\n",
      "epoch: 21000, loss: 0.33627288024412616, acc: 0.8761904761904762\n",
      "epoch: 21100, loss: 0.33627286442845283, acc: 0.8761904761904762\n",
      "epoch: 21200, loss: 0.33627284938083457, acc: 0.8761904761904762\n",
      "epoch: 21300, loss: 0.3362728350639004, acc: 0.8761904761904762\n",
      "epoch: 21400, loss: 0.3362728214421026, acc: 0.8761904761904762\n",
      "epoch: 21500, loss: 0.3362728084816276, acc: 0.8761904761904762\n",
      "epoch: 21600, loss: 0.33627279615031136, acc: 0.8761904761904762\n",
      "epoch: 21700, loss: 0.33627278441755815, acc: 0.8761904761904762\n",
      "epoch: 21800, loss: 0.33627277325426397, acc: 0.8761904761904762\n",
      "epoch: 21900, loss: 0.33627276263274364, acc: 0.8761904761904762\n",
      "epoch: 22000, loss: 0.33627275252666095, acc: 0.8761904761904762\n",
      "epoch: 22100, loss: 0.3362727429109634, acc: 0.8761904761904762\n",
      "epoch: 22200, loss: 0.3362727337618187, acc: 0.8761904761904762\n",
      "epoch: 22300, loss: 0.3362727250565556, acc: 0.8761904761904762\n",
      "epoch: 22400, loss: 0.3362727167736068, acc: 0.8761904761904762\n",
      "epoch: 22500, loss: 0.3362727088924556, acc: 0.8761904761904762\n",
      "epoch: 22600, loss: 0.33627270139358356, acc: 0.8761904761904762\n",
      "epoch: 22700, loss: 0.3362726942584229, acc: 0.8761904761904762\n",
      "epoch: 22800, loss: 0.33627268746930905, acc: 0.8761904761904762\n",
      "epoch: 22900, loss: 0.3362726810094375, acc: 0.8761904761904762\n",
      "epoch: 23000, loss: 0.33627267486282103, acc: 0.8761904761904762\n",
      "epoch: 23100, loss: 0.33627266901425057, acc: 0.8761904761904762\n",
      "epoch: 23200, loss: 0.33627266344925627, acc: 0.8761904761904762\n",
      "epoch: 23300, loss: 0.3362726581540726, acc: 0.8761904761904762\n",
      "epoch: 23400, loss: 0.33627265311560284, acc: 0.8761904761904762\n",
      "epoch: 23500, loss: 0.3362726483213876, acc: 0.8761904761904762\n",
      "epoch: 23600, loss: 0.3362726437595729, acc: 0.8761904761904762\n",
      "epoch: 23700, loss: 0.336272639418881, acc: 0.8761904761904762\n",
      "epoch: 23800, loss: 0.3362726352885825, acc: 0.8761904761904762\n",
      "epoch: 23900, loss: 0.336272631358469, acc: 0.8761904761904762\n",
      "epoch: 24000, loss: 0.3362726276188288, acc: 0.8761904761904762\n",
      "epoch: 24100, loss: 0.33627262406042147, acc: 0.8761904761904762\n",
      "epoch: 24200, loss: 0.3362726206744556, acc: 0.8761904761904762\n",
      "epoch: 24300, loss: 0.33627261745256704, acc: 0.8761904761904762\n",
      "epoch: 24400, loss: 0.33627261438679795, acc: 0.8761904761904762\n",
      "epoch: 24500, loss: 0.33627261146957654, acc: 0.8761904761904762\n",
      "epoch: 24600, loss: 0.3362726086936991, acc: 0.8761904761904762\n",
      "epoch: 24700, loss: 0.3362726060523115, acc: 0.8761904761904762\n",
      "epoch: 24800, loss: 0.3362726035388927, acc: 0.8761904761904762\n",
      "epoch: 24900, loss: 0.3362726011472379, acc: 0.8761904761904762\n",
      "epoch: 25000, loss: 0.33627259887144345, acc: 0.8761904761904762\n",
      "epoch: 25100, loss: 0.33627259670589266, acc: 0.8761904761904762\n",
      "epoch: 25200, loss: 0.336272594645241, acc: 0.8761904761904762\n",
      "epoch: 25300, loss: 0.33627259268440374, acc: 0.8761904761904762\n",
      "epoch: 25400, loss: 0.33627259081854244, acc: 0.8761904761904762\n",
      "epoch: 25500, loss: 0.33627258904305374, acc: 0.8761904761904762\n",
      "epoch: 25600, loss: 0.33627258735355764, acc: 0.8761904761904762\n",
      "epoch: 25700, loss: 0.33627258574588625, acc: 0.8761904761904762\n",
      "epoch: 25800, loss: 0.33627258421607453, acc: 0.8761904761904762\n",
      "epoch: 25900, loss: 0.33627258276034927, acc: 0.8761904761904762\n",
      "epoch: 26000, loss: 0.3362725813751203, acc: 0.8761904761904762\n",
      "epoch: 26100, loss: 0.3362725800569718, acc: 0.8761904761904762\n",
      "epoch: 26200, loss: 0.33627257880265343, acc: 0.8761904761904762\n",
      "epoch: 26300, loss: 0.33627257760907264, acc: 0.8761904761904762\n",
      "epoch: 26400, loss: 0.33627257647328657, acc: 0.8761904761904762\n",
      "epoch: 26500, loss: 0.3362725753924954, acc: 0.8761904761904762\n",
      "epoch: 26600, loss: 0.33627257436403485, acc: 0.8761904761904762\n",
      "epoch: 26700, loss: 0.3362725733853699, acc: 0.8761904761904762\n",
      "epoch: 26800, loss: 0.33627257245408815, acc: 0.8761904761904762\n",
      "epoch: 26900, loss: 0.3362725715678945, acc: 0.8761904761904762\n",
      "epoch: 27000, loss: 0.3362725707246051, acc: 0.8761904761904762\n",
      "epoch: 27100, loss: 0.3362725699221418, acc: 0.8761904761904762\n",
      "epoch: 27200, loss: 0.336272569158527, acc: 0.8761904761904762\n",
      "epoch: 27300, loss: 0.3362725684318792, acc: 0.8761904761904762\n",
      "epoch: 27400, loss: 0.33627256774040837, acc: 0.8761904761904762\n",
      "epoch: 27500, loss: 0.3362725670824105, acc: 0.8761904761904762\n",
      "epoch: 27600, loss: 0.3362725664562648, acc: 0.8761904761904762\n",
      "epoch: 27700, loss: 0.3362725658604286, acc: 0.8761904761904762\n",
      "epoch: 27800, loss: 0.3362725652934343, acc: 0.8761904761904762\n",
      "epoch: 27900, loss: 0.33627256475388523, acc: 0.8761904761904762\n",
      "epoch: 28000, loss: 0.3362725642404524, acc: 0.8761904761904762\n",
      "epoch: 28100, loss: 0.33627256375187115, acc: 0.8761904761904762\n",
      "epoch: 28200, loss: 0.33627256328693833, acc: 0.8761904761904762\n",
      "epoch: 28300, loss: 0.3362725628445088, acc: 0.8761904761904762\n",
      "epoch: 28400, loss: 0.33627256242349307, acc: 0.8761904761904762\n",
      "epoch: 28500, loss: 0.3362725620228543, acc: 0.8761904761904762\n",
      "epoch: 28600, loss: 0.3362725616416059, acc: 0.8761904761904762\n",
      "epoch: 28700, loss: 0.33627256127880933, acc: 0.8761904761904762\n",
      "epoch: 28800, loss: 0.3362725609335711, acc: 0.8761904761904762\n",
      "epoch: 28900, loss: 0.33627256060504124, acc: 0.8761904761904762\n",
      "epoch: 29000, loss: 0.3362725602924109, acc: 0.8761904761904762\n",
      "epoch: 29100, loss: 0.33627255999491035, acc: 0.8761904761904762\n",
      "epoch: 29200, loss: 0.33627255971180714, acc: 0.8761904761904762\n",
      "epoch: 29300, loss: 0.33627255944240453, acc: 0.8761904761904762\n",
      "epoch: 29400, loss: 0.3362725591860391, acc: 0.8761904761904762\n",
      "epoch: 29500, loss: 0.33627255894207986, acc: 0.8761904761904762\n",
      "epoch: 29600, loss: 0.3362725587099264, acc: 0.8761904761904762\n",
      "epoch: 29700, loss: 0.33627255848900706, acc: 0.8761904761904762\n",
      "epoch: 29800, loss: 0.3362725582787782, acc: 0.8761904761904762\n",
      "epoch: 29900, loss: 0.3362725580787224, acc: 0.8761904761904762\n",
      "epoch: 30000, loss: 0.3362725578883473, acc: 0.8761904761904762\n",
      "epoch: 30100, loss: 0.3362725577071842, acc: 0.8761904761904762\n",
      "epoch: 30200, loss: 0.33627255753478746, acc: 0.8761904761904762\n",
      "epoch: 30300, loss: 0.3362725573707325, acc: 0.8761904761904762\n",
      "epoch: 30400, loss: 0.33627255721461596, acc: 0.8761904761904762\n",
      "epoch: 30500, loss: 0.33627255706605336, acc: 0.8761904761904762\n",
      "epoch: 30600, loss: 0.33627255692467917, acc: 0.8761904761904762\n",
      "epoch: 30700, loss: 0.3362725567901456, acc: 0.8761904761904762\n",
      "epoch: 30800, loss: 0.3362725566621217, acc: 0.8761904761904762\n",
      "epoch: 30900, loss: 0.3362725565402921, acc: 0.8761904761904762\n",
      "epoch: 31000, loss: 0.3362725564243572, acc: 0.8761904761904762\n",
      "epoch: 31100, loss: 0.3362725563140318, acc: 0.8761904761904762\n",
      "epoch: 31200, loss: 0.3362725562090445, acc: 0.8761904761904762\n",
      "epoch: 31300, loss: 0.3362725561091367, acc: 0.8761904761904762\n",
      "epoch: 31400, loss: 0.33627255601406286, acc: 0.8761904761904762\n",
      "epoch: 31500, loss: 0.33627255592358896, acc: 0.8761904761904762\n",
      "epoch: 31600, loss: 0.33627255583749244, acc: 0.8761904761904762\n",
      "epoch: 31700, loss: 0.33627255575556136, acc: 0.8761904761904762\n",
      "epoch: 31800, loss: 0.33627255567759434, acc: 0.8761904761904762\n",
      "epoch: 31900, loss: 0.33627255560339947, acc: 0.8761904761904762\n",
      "epoch: 32000, loss: 0.3362725555327942, acc: 0.8761904761904762\n",
      "epoch: 32100, loss: 0.3362725554656049, acc: 0.8761904761904762\n",
      "epoch: 32200, loss: 0.33627255540166623, acc: 0.8761904761904762\n",
      "epoch: 32300, loss: 0.33627255534082096, acc: 0.8761904761904762\n",
      "epoch: 32400, loss: 0.3362725552829194, acc: 0.8761904761904762\n",
      "epoch: 32500, loss: 0.33627255522781896, acc: 0.8761904761904762\n",
      "epoch: 32600, loss: 0.3362725551753844, acc: 0.8761904761904762\n",
      "epoch: 32700, loss: 0.33627255512548654, acc: 0.8761904761904762\n",
      "epoch: 32800, loss: 0.33627255507800263, acc: 0.8761904761904762\n",
      "epoch: 32900, loss: 0.3362725550328159, acc: 0.8761904761904762\n",
      "epoch: 33000, loss: 0.3362725549898153, acc: 0.8761904761904762\n",
      "epoch: 33100, loss: 0.3362725549488949, acc: 0.8761904761904762\n",
      "epoch: 33200, loss: 0.3362725549099542, acc: 0.8761904761904762\n",
      "epoch: 33300, loss: 0.33627255487289737, acc: 0.8761904761904762\n",
      "epoch: 33400, loss: 0.33627255483763313, acc: 0.8761904761904762\n",
      "epoch: 33500, loss: 0.33627255480407503, acc: 0.8761904761904762\n",
      "epoch: 33600, loss: 0.33627255477214024, acc: 0.8761904761904762\n",
      "epoch: 33700, loss: 0.3362725547417504, acc: 0.8761904761904762\n",
      "epoch: 33800, loss: 0.3362725547128307, acc: 0.8761904761904762\n",
      "epoch: 33900, loss: 0.33627255468531003, acc: 0.8761904761904762\n",
      "epoch: 34000, loss: 0.3362725546591207, acc: 0.8761904761904762\n",
      "epoch: 34100, loss: 0.33627255463419825, acc: 0.8761904761904762\n",
      "epoch: 34200, loss: 0.3362725546104815, acc: 0.8761904761904762\n",
      "epoch: 34300, loss: 0.336272554587912, acc: 0.8761904761904762\n",
      "epoch: 34400, loss: 0.33627255456643435, acc: 0.8761904761904762\n",
      "epoch: 34500, loss: 0.33627255454599564, acc: 0.8761904761904762\n",
      "epoch: 34600, loss: 0.33627255452654564, acc: 0.8761904761904762\n",
      "epoch: 34700, loss: 0.3362725545080366, acc: 0.8761904761904762\n",
      "epoch: 34800, loss: 0.33627255449042276, acc: 0.8761904761904762\n",
      "epoch: 34900, loss: 0.3362725544736612, acc: 0.8761904761904762\n",
      "epoch: 35000, loss: 0.3362725544577104, acc: 0.8761904761904762\n",
      "epoch: 35100, loss: 0.33627255444253107, acc: 0.8761904761904762\n",
      "epoch: 35200, loss: 0.3362725544280861, acc: 0.8761904761904762\n",
      "epoch: 35300, loss: 0.3362725544143399, acc: 0.8761904761904762\n",
      "epoch: 35400, loss: 0.33627255440125864, acc: 0.8761904761904762\n",
      "epoch: 35500, loss: 0.3362725543888104, acc: 0.8761904761904762\n",
      "epoch: 35600, loss: 0.336272554376964, acc: 0.8761904761904762\n",
      "epoch: 35700, loss: 0.3362725543656907, acc: 0.8761904761904762\n",
      "epoch: 35800, loss: 0.33627255435496284, acc: 0.8761904761904762\n",
      "epoch: 35900, loss: 0.3362725543447539, acc: 0.8761904761904762\n",
      "epoch: 36000, loss: 0.33627255433503866, acc: 0.8761904761904762\n",
      "epoch: 36100, loss: 0.3362725543257934, acc: 0.8761904761904762\n",
      "epoch: 36200, loss: 0.3362725543169955, acc: 0.8761904761904762\n",
      "epoch: 36300, loss: 0.33627255430862296, acc: 0.8761904761904762\n",
      "epoch: 36400, loss: 0.3362725543006556, acc: 0.8761904761904762\n",
      "epoch: 36500, loss: 0.3362725542930737, acc: 0.8761904761904762\n",
      "epoch: 36600, loss: 0.3362725542858584, acc: 0.8761904761904762\n",
      "epoch: 36700, loss: 0.3362725542789922, acc: 0.8761904761904762\n",
      "epoch: 36800, loss: 0.3362725542724581, acc: 0.8761904761904762\n",
      "epoch: 36900, loss: 0.33627255426624, acc: 0.8761904761904762\n",
      "epoch: 37000, loss: 0.33627255426032276, acc: 0.8761904761904762\n",
      "epoch: 37100, loss: 0.3362725542546917, acc: 0.8761904761904762\n",
      "epoch: 37200, loss: 0.33627255424933306, acc: 0.8761904761904762\n",
      "epoch: 37300, loss: 0.3362725542442336, acc: 0.8761904761904762\n",
      "epoch: 37400, loss: 0.33627255423938085, acc: 0.8761904761904762\n",
      "epoch: 37500, loss: 0.3362725542347627, acc: 0.8761904761904762\n",
      "epoch: 37600, loss: 0.3362725542303681, acc: 0.8761904761904762\n",
      "epoch: 37700, loss: 0.336272554226186, acc: 0.8761904761904762\n",
      "epoch: 37800, loss: 0.33627255422220625, acc: 0.8761904761904762\n",
      "epoch: 37900, loss: 0.3362725542184189, acc: 0.8761904761904762\n",
      "epoch: 38000, loss: 0.33627255421481483, acc: 0.8761904761904762\n",
      "epoch: 38100, loss: 0.3362725542113851, acc: 0.8761904761904762\n",
      "epoch: 38200, loss: 0.3362725542081212, acc: 0.8761904761904762\n",
      "epoch: 38300, loss: 0.33627255420501523, acc: 0.8761904761904762\n",
      "epoch: 38400, loss: 0.3362725542020595, acc: 0.8761904761904762\n",
      "epoch: 38500, loss: 0.3362725541992466, acc: 0.8761904761904762\n",
      "epoch: 38600, loss: 0.33627255419656993, acc: 0.8761904761904762\n",
      "epoch: 38700, loss: 0.33627255419402263, acc: 0.8761904761904762\n",
      "epoch: 38800, loss: 0.33627255419159874, acc: 0.8761904761904762\n",
      "epoch: 38900, loss: 0.33627255418929186, acc: 0.8761904761904762\n",
      "epoch: 39000, loss: 0.3362725541870967, acc: 0.8761904761904762\n",
      "epoch: 39100, loss: 0.3362725541850076, acc: 0.8761904761904762\n",
      "epoch: 39200, loss: 0.33627255418301966, acc: 0.8761904761904762\n",
      "epoch: 39300, loss: 0.33627255418112784, acc: 0.8761904761904762\n",
      "epoch: 39400, loss: 0.33627255417932755, acc: 0.8761904761904762\n",
      "epoch: 39500, loss: 0.3362725541776143, acc: 0.8761904761904762\n",
      "epoch: 39600, loss: 0.33627255417598395, acc: 0.8761904761904762\n",
      "epoch: 39700, loss: 0.33627255417443247, acc: 0.8761904761904762\n",
      "epoch: 39800, loss: 0.336272554172956, acc: 0.8761904761904762\n",
      "epoch: 39900, loss: 0.3362725541715509, acc: 0.8761904761904762\n",
      "epoch: 40000, loss: 0.3362725541702138, acc: 0.8761904761904762\n",
      "epoch: 40100, loss: 0.3362725541689414, acc: 0.8761904761904762\n",
      "epoch: 40200, loss: 0.3362725541677306, acc: 0.8761904761904762\n",
      "epoch: 40300, loss: 0.3362725541665783, acc: 0.8761904761904762\n",
      "epoch: 40400, loss: 0.3362725541654817, acc: 0.8761904761904762\n",
      "epoch: 40500, loss: 0.33627255416443813, acc: 0.8761904761904762\n",
      "epoch: 40600, loss: 0.33627255416344515, acc: 0.8761904761904762\n",
      "epoch: 40700, loss: 0.3362725541625001, acc: 0.8761904761904762\n",
      "epoch: 40800, loss: 0.3362725541616008, acc: 0.8761904761904762\n",
      "epoch: 40900, loss: 0.33627255416074503, acc: 0.8761904761904762\n",
      "epoch: 41000, loss: 0.3362725541599305, acc: 0.8761904761904762\n",
      "epoch: 41100, loss: 0.3362725541591556, acc: 0.8761904761904762\n",
      "epoch: 41200, loss: 0.3362725541584181, acc: 0.8761904761904762\n",
      "epoch: 41300, loss: 0.3362725541577163, acc: 0.8761904761904762\n",
      "epoch: 41400, loss: 0.3362725541570483, acc: 0.8761904761904762\n",
      "epoch: 41500, loss: 0.3362725541564127, acc: 0.8761904761904762\n",
      "epoch: 41600, loss: 0.33627255415580776, acc: 0.8761904761904762\n",
      "epoch: 41700, loss: 0.3362725541552323, acc: 0.8761904761904762\n",
      "epoch: 41800, loss: 0.3362725541546845, acc: 0.8761904761904762\n",
      "epoch: 41900, loss: 0.33627255415416324, acc: 0.8761904761904762\n",
      "epoch: 42000, loss: 0.3362725541536672, acc: 0.8761904761904762\n",
      "epoch: 42100, loss: 0.336272554153195, acc: 0.8761904761904762\n",
      "epoch: 42200, loss: 0.3362725541527459, acc: 0.8761904761904762\n",
      "epoch: 42300, loss: 0.3362725541523184, acc: 0.8761904761904762\n",
      "epoch: 42400, loss: 0.33627255415191165, acc: 0.8761904761904762\n",
      "epoch: 42500, loss: 0.33627255415152446, acc: 0.8761904761904762\n",
      "epoch: 42600, loss: 0.3362725541511559, acc: 0.8761904761904762\n",
      "epoch: 42700, loss: 0.3362725541508054, acc: 0.8761904761904762\n",
      "epoch: 42800, loss: 0.3362725541504718, acc: 0.8761904761904762\n",
      "epoch: 42900, loss: 0.3362725541501543, acc: 0.8761904761904762\n",
      "epoch: 43000, loss: 0.3362725541498522, acc: 0.8761904761904762\n",
      "epoch: 43100, loss: 0.3362725541495646, acc: 0.8761904761904762\n",
      "epoch: 43200, loss: 0.336272554149291, acc: 0.8761904761904762\n",
      "epoch: 43300, loss: 0.3362725541490306, acc: 0.8761904761904762\n",
      "epoch: 43400, loss: 0.3362725541487829, acc: 0.8761904761904762\n",
      "epoch: 43500, loss: 0.33627255414854695, acc: 0.8761904761904762\n",
      "epoch: 43600, loss: 0.3362725541483226, acc: 0.8761904761904762\n",
      "epoch: 43700, loss: 0.336272554148109, acc: 0.8761904761904762\n",
      "epoch: 43800, loss: 0.33627255414790586, acc: 0.8761904761904762\n",
      "epoch: 43900, loss: 0.33627255414771245, acc: 0.8761904761904762\n",
      "epoch: 44000, loss: 0.33627255414752844, acc: 0.8761904761904762\n",
      "epoch: 44100, loss: 0.3362725541473533, acc: 0.8761904761904762\n",
      "epoch: 44200, loss: 0.33627255414718665, acc: 0.8761904761904762\n",
      "epoch: 44300, loss: 0.33627255414702795, acc: 0.8761904761904762\n",
      "epoch: 44400, loss: 0.33627255414687707, acc: 0.8761904761904762\n",
      "epoch: 44500, loss: 0.3362725541467334, acc: 0.8761904761904762\n",
      "epoch: 44600, loss: 0.3362725541465968, acc: 0.8761904761904762\n",
      "epoch: 44700, loss: 0.33627255414646673, acc: 0.8761904761904762\n",
      "epoch: 44800, loss: 0.33627255414634294, acc: 0.8761904761904762\n",
      "Validation Accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "sample_num = 150\n",
    "feature_num = 10\n",
    "test_sample_size = 0.3\n",
    "max_epoch = 300000\n",
    "loss_threshold = 1e-15\n",
    "learn_step = 0.01   # 一般取值1e-2 ~ 1e-3\n",
    "\n",
    "\n",
    "# 准备数据集, 并在数据集的前面增加1列全1列\n",
    "X, y = make_classification(sample_num, feature_num) \n",
    "column_ones = np.ones((X.shape[0], 1))          # 参数是一个元组，表示行和列\n",
    "X = np.concatenate((column_ones, X), axis=1)        # axis=1水平方向连接， axis=0 竖直...\n",
    "\n",
    "# 拆分数据集: 训练数据集、测试数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sample_size)\n",
    "\n",
    "\n",
    "# 初始化模型参数, 注意要包含偏置项bias\n",
    "theta = np.random.randn(1, feature_num + 1) \n",
    "theta = np.zeros(feature_num + 1)\n",
    "\n",
    "# 训练模型\n",
    "theta = model_train(X_train, y_train, theta, learn_step, max_epoch, loss_threshold)\n",
    "\n",
    "# 验证模型\n",
    "model_verify(X_test, y_test, theta)\n",
    "\n",
    "# 保存模型参数\n",
    "np.save('theta.npy', theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49516105  2.45709664 -0.27988594 -0.36977526  0.1542271  -0.1644425\n",
      " -0.23786215  0.23505187 -0.43441541  0.13597378 -0.0729565 ]\n",
      "Validation Accuracy: 0.4266666666666667\n"
     ]
    }
   ],
   "source": [
    "# 从文件中加载模型参数\n",
    "theta = np.load('theta.npy')\n",
    "print(theta)\n",
    "\n",
    "# 准备数据集, 并在数据集的前面增加1列全1列\n",
    "feature_num = 10       # 样本的数量\n",
    "sample_num = 150       # 样本特征的个数\n",
    "X, y = make_classification(sample_num, feature_num) \n",
    "column_ones = np.ones((X.shape[0], 1))          # 参数是一个元组，表示行和列\n",
    "X = np.concatenate((column_ones, X), axis=1)        # axis=1水平方向连接， axis=0 竖直...\n",
    "\n",
    "# 使用模型参数，计算所有样本是正类的预测概率\n",
    "z = np.dot(X, theta.T)\n",
    "y_hat = 1 / (1 + np.exp(-z))\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = calc_accuracy(y_hat, y)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
